{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31379a55-3a03-4b93-893b-c5d21564d985",
   "metadata": {},
   "source": [
    "# Random Forest Model (California Housing Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5140e2c-bfd0-45ed-950a-63a5769339c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor       # Import the Random Forest Regressor\n",
    "from sklearn.model_selection import train_test_split     # Good practice for train/test split\n",
    "from sklearn.metrics import mean_squared_error, r2_score # For evaluating the model\n",
    "import time # For timing\n",
    "import math # For math.ceil\n",
    "import copy # For copy.deepcopy\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb206989-7b5d-45a0-acff-6a121f598662",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This notebook demonstrates the implementation and evaluation of a Random Forest Regressor for predicting California house values. It builds upon previous analyses (e.g., Linear Regression) by applying targeted feature engineering and leveraging a more flexible, non-linear model to capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b984fa9-0c9f-422a-b9e7-08b9a5b8f412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. Data Loading and Initial Preparation --- \n",
    "# Load the California Housing dataset. This dataset is suitable for regression tasks.\n",
    "housing = fetch_california_housing()\n",
    "# Create a Pandas DataFrame from the features, using the feature names for columns.\n",
    "california_df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "# Add the target variable (Median House Value) to the DataFrame.\n",
    "california_df['MedHouseVal'] = housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece7c67-bc25-440d-912f-8672ad8eada2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the initial set of features to be used. These were selected based on\n",
    "# common sense and initial exploratory data analysis (e.g., from previous linear regression attempts).\n",
    "\n",
    "initial_feature_names_rf = [\n",
    "    'MedInc',      # Median Income in block group\n",
    "    'HouseAge',    # Median house age in block group\n",
    "    'AveRooms',    # Average number of rooms per household\n",
    "    'Population'   # Block group population\n",
    "]\n",
    "\n",
    "# Create the base feature matrix (X) and target vector (y) for the Random Forest model.\n",
    "# X_train_rf will be further modified with engineered features.\n",
    "X_train_rf = california_df[initial_feature_names_rf].values\n",
    "y_train_rf = california_df['MedHouseVal'].values\n",
    "\n",
    "# This list will dynamically track all features used in the RF model for plotting labels\n",
    "X_features_rf = list(initial_feature_names_rf)\n",
    "#print(f\"Base X_train_rf shape (original features): {X_train_rf.shape}\")\n",
    "\n",
    "# --- 2. Feature Engineering for Random Forest ---\n",
    "# Random Forests are powerful because they can capture non-linearity and interactions\n",
    "# automatically. However, well-engineered features can still significantly improve\n",
    "# their performance and help them discover patterns more easily.\n",
    "# Importantly, tree-based models like Random Forests do NOT require feature scaling/normalization. \n",
    "\n",
    "#print(\"\\n2. Performing Feature Engineering (Polynomial, Logarithmic, Interaction) for Random Forest...\")\n",
    "\n",
    "# --- Store original column indices for plotting later ---\n",
    "# We need to know the original index of certain features in the initial_feature_names_rf list\n",
    "# before we start adding new columns, as concatenation changes column positions.\n",
    "medinc_original_column_index_rf = X_features_rf.index('MedInc')\n",
    "population_original_column_index_rf = X_features_rf.index('Population')\n",
    "averooms_original_column_index_rf = X_features_rf.index('AveRooms')\n",
    "\n",
    "\n",
    "# --- Feature 2.1: Add MedInc_Sq (Polynomial Feature) ---\n",
    "# Rationale: Initial Linear Regression analysis showed 'MedInc' had a non-linear \n",
    "# relationship with the target, which linear models struggle with. Adding a squared term\n",
    "# allows the model to approximate this curve.\n",
    "medinc_column_data_rf = X_train_rf[:, medinc_original_column_index_rf]\n",
    "medinc_squared_data_rf = medinc_column_data_rf**2\n",
    "X_features_rf.append('MedInc_Sq') # Add the new feature name to our list\n",
    "X_train_rf = np.c_[X_train_rf, medinc_squared_data_rf] # Concatenate as a new column\n",
    "\n",
    "\n",
    "# --- Feature 2.2: Add Log_Population (Logarithmic Transformation) ---\n",
    "# Rationale: 'Population' often has a highly skewed distribution (many small values, few large).\n",
    "# Log transforms can make skewed distributions more symmetrical and relationships more linear-like,\n",
    "# benefiting many models, even Random Forests, by normalizing the spread.\n",
    "# Using np.log1p (log(1+x)) is safer as it handles potential zero values gracefully.\n",
    "population_column_data_rf = X_train_rf[:, population_original_column_index_rf]\n",
    "log_population_data_rf = np.log1p(population_column_data_rf)\n",
    "X_features_rf.append('Log_Population')\n",
    "X_train_rf = np.c_[X_train_rf, log_population_data_rf]\n",
    "\n",
    "\n",
    "# --- Feature 2.3: Add MedInc_x_AveRooms (Interaction Feature) ---\n",
    "# Rationale: An interaction term captures how the effect of one feature might depend\n",
    "# on the value of another. For example, the impact of average rooms might be different\n",
    "# at different median income levels.\n",
    "# It's best practice to use the original, untransformed features for interaction terms.\n",
    "medinc_x_averooms_data_rf = california_df['MedInc'].values * california_df['AveRooms'].values\n",
    "X_features_rf.append('MedInc_x_AveRooms')\n",
    "X_train_rf = np.c_[X_train_rf, medinc_x_averooms_data_rf]\n",
    "\n",
    "\n",
    "print(f\"X_train_rf shape after adding all new features: {X_train_rf.shape}\")\n",
    "print(f\"Final feature list for RF model: {X_features_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941a1dd-fe77-4a41-94cd-06ad2f4d3651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 3. Split Data into Training and Testing Sets ---\n",
    "# This is a crucial step for robust model evaluation. It helps assess how well the model\n",
    "# generalizes to unseen data, preventing overfitting.\n",
    "print(\"\\n3. Splitting data into training (80%) and testing (20%) sets...\")\n",
    "\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
    "    X_train_rf, y_train_rf, test_size=0.2, random_state=42 # random_state for reproducibility\n",
    ")\n",
    "print(f\"X_train_split shape: {X_train_split.shape}\")\n",
    "print(f\"X_test_split shape: {X_test_split.shape}\")\n",
    "\n",
    "\n",
    "# --- 4. Initialize and Train the Random Forest Regressor ---\n",
    "print(\"\\n4. Initializing and Training Random Forest Regressor...\")\n",
    "# RandomForestRegressor: An ensemble learning method for regression. It constructs\n",
    "# a multitude of decision trees at training time and outputs the average prediction\n",
    "# of the individual trees.\n",
    "# n_estimators: The number of trees in the forest. More trees generally improve accuracy\n",
    "#               but increase computation time. 100 is a common starting point.\n",
    "# random_state: Controls the randomness of the bootstrapping of the samples and\n",
    "#               the splitting criteria for each tree. Ensures reproducible results.\n",
    "# n_jobs=-1: Tells scikit-learn to use all available CPU cores for parallel processing,\n",
    "#            which significantly speeds up training for larger datasets/models.\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the model using the training data.\n",
    "print(\"Training the Random Forest model...\")\n",
    "tic_rf = time.time()\n",
    "rf_model.fit(X_train_split, y_train_split)\n",
    "toc_rf = time.time()\n",
    "print(f\"Random Forest training duration: {1000*(toc_rf-tic_rf):.4f} ms \")\n",
    "\n",
    "# Save the trained model to a .pkl file\n",
    "# This will create a file named 'random_forest_model.pkl' in the same directory (Can be used by an API later)\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "joblib.dump(X_features_rf, 'feature_names.pkl')\n",
    "# --- 5. Make Predictions and Evaluate the Model ---\n",
    "#print(\"\\n5. Making predictions and evaluating the model...\")\n",
    "\n",
    "# Make predictions on both the training and testing sets.\n",
    "# Predictions on the test set are critical for assessing generalization performance.\n",
    "yp_train_rf = rf_model.predict(X_train_split)\n",
    "yp_test_rf = rf_model.predict(X_test_split)\n",
    "\n",
    "# Evaluate the model using Root Mean Squared Error (RMSE) and R-squared (R2).\n",
    "# RMSE: Measures the average magnitude of the errors. (Lower is better).\n",
    "# R-squared: Explains the proportion of variance in the dependent variable\n",
    "#            that can be predicted from the independent variables. Higher (closer to 1) is better.\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_split, yp_train_rf))\n",
    "train_r2 = r2_score(y_train_split, yp_train_rf)\n",
    "print(f\"Random Forest (Train Set) RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Random Forest (Train Set) R-squared: {train_r2:.4f}\")\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_split, yp_test_rf))\n",
    "test_r2 = r2_score(y_test_split, yp_test_rf)\n",
    "print(f\"Random Forest (Test Set) RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Random Forest (Test Set) R-squared: {test_r2:.4f}\")\n",
    "\n",
    "# --- Interpretation of Evaluation ---\n",
    "# If Train R2 is much higher than Test R2, it suggests overfitting.\n",
    "# If both are high, it suggests good generalization.\n",
    "\n",
    "\n",
    "# --- 6. Plot Predictions vs. Target for Key Features ---\n",
    "# Visualizing predictions against original features helps in understanding\n",
    "# how well the model captures underlying patterns, especially non-linear ones.\n",
    "print(\"\\n6. Plotting Random Forest predictions vs. original features...\")\n",
    "\n",
    "# Define custom colours for plotting\n",
    "TARGET_COLOR = 'mediumseagreen' # For the actual target values\n",
    "PREDICT_COLOR = 'rebeccapurple' # For the model's predictions\n",
    "\n",
    "# To visualize predictions on the full dataset (as in previous LR plots),\n",
    "# we predict on the original full X_train_rf (which has all features).\n",
    "yp_rf_full = rf_model.predict(X_train_rf)\n",
    "\n",
    "# Plotting the initial 4 base features (MedInc, HouseAge, AveRooms, Population)\n",
    "# The predictions now incorporate the effect of all 7 features in the RF model.\n",
    "fig,ax=plt.subplots(1,len(initial_feature_names_rf),figsize=(16, 4),sharey=True)\n",
    "for i, feature_name in enumerate(initial_feature_names_rf):\n",
    "    ax[i].scatter(X_train_rf[:,i], y_train_rf, label = 'Target', s=5, alpha=0.5, color=TARGET_COLOR)\n",
    "    ax[i].scatter(X_train_rf[:,i], yp_rf_full, label = 'Predict (RF)', s=5, alpha=0.7, color=PREDICT_COLOR)\n",
    "    ax[i].set_xlabel(feature_name)\n",
    "    ax[i].set_title(feature_name)\n",
    "\n",
    "ax[0].set_ylabel(\"Median House Value\");\n",
    "ax[0].legend(); # Legend is added only to the first subplot due to sharey=True\n",
    "fig.suptitle(\"Target vs. Prediction: Random Forest Regressor (on original features)\", y=1.02)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# --- Crucial plot: MedInc (original) vs. predictions with Random Forest ---\n",
    "# This plot is key to visually assessing how well the Random Forest handles\n",
    "# the previously observed non-linearity and varying dispersion in 'MedInc'.\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_train_rf[:, medinc_original_column_index_rf], y_train_rf, label='Target', s=5, alpha=0.5, color=TARGET_COLOR)\n",
    "plt.scatter(X_train_rf[:, medinc_original_column_index_rf], yp_rf_full, label='Predict (RF)', s=5, alpha=0.7, color=PREDICT_COLOR)\n",
    "plt.xlabel(\"MedInc (Original)\")\n",
    "plt.ylabel(\"Median House Value\")\n",
    "plt.title(f\"MedInc (Original) vs. Target/Prediction (Random Forest)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n--- End of Random Forest Analysis ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113f91e-f7a7-4382-b3be-2ee7209c6297",
   "metadata": {},
   "source": [
    "## 2. Conclusion\n",
    "\n",
    "The Random Forest Regressor demonstrated a significant improvement over the linear regression models. Its ability to handle non-linear relationships allowed it to capture the complex patterns in the California housing data much more effectively, resulting in superior predictive performance.\n",
    "\n",
    "**Key advantages of the Random Forest model observed in this analysis:**\n",
    "\n",
    "- Handling Non-Linearity: Unlike linear regression, Random Forest inherently models non-linear relationships, leading to more accurate predictions, particularly for features like 'MedInc'.\n",
    "\n",
    "- Computational Efficiency: The Random Forest model trained relatively quickly, especially compared to the iterative process of feature engineering and Gradient Descent required for linear regression.\n",
    "\n",
    "- No Normalization Required: Random Forest models are tree-based and thus invariant to feature scaling, eliminating the need for normalization and simplifying the data preparation process.\n",
    "\n",
    "The success of the Random Forest Regressor in this project makes it an excellent candidate for deployment. To demonstrate this capability and make the model readily accessible, the next step will be to create a simple API (using FastAPI) to serve predictions based on user-provided housing features. This API deployment will be detailed in a separate project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
