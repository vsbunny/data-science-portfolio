{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597861e9-74a2-4156-bbf3-f0aa2ee619bf",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent (California Housing Dataset) - One Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721f1fc-aefb-43e7-a0d7-8afb5186a445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60813d33-ea7c-47f8-be9c-8a3949156745",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "This notebook implements linear regression using gradient descent to predict median house values in California, based on the 'MedInc' (median income) feature from the California Housing dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4042954-17c5-4513-b1a3-f77bd7ef657e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the California Housing dataset. This dataset is suitable for regression tasks.\n",
    "housing = fetch_california_housing()\n",
    "# Create a Pandas DataFrame from the features, using the feature names for columns.\n",
    "california_df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "# Add the target variable (Median House Value) to the DataFrame.\n",
    "california_df['MedHouseVal'] = housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019b760-dd37-4f26-949d-d86eaf76f610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "california_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40e7be-62de-495b-9b81-5370a8aed2d6",
   "metadata": {},
   "source": [
    "### 2. Data Exploration\n",
    "\n",
    "The California Housing dataset contains information about housing prices in California districts. Each row represents a census block group, and the columns describe various attributes of that region. We will explore the relationship between the features and the target variable, Median House Value ('MedHouseVal'), to determine which features are most relevant for predicting house prices.\n",
    "\n",
    "The 'MedInc' feature represents the median income in each block group, measured in tens of thousands of dollars. We expect that 'MedInc' will have a positive correlation with 'MedHouseVal', as areas with higher median incomes typically have higher property values.\n",
    "\n",
    "To understand the relationship between the features and the target variable (MedHouseVal), we use a scatter plot matrix. This helps us identify potential linear relationships that are suitable for linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0efafa-af3b-40b7-a135-64b4816bac18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize linearity: A scatter plot matrix is generated to assess the relationships between numerical variables.  This helps determine if linear regression is appropriate, as the model assumes a linear relationship between the independent and dependent variables.\n",
    "def plotScatterMatrix(df, plotSize, textSize):\n",
    "    df = california_df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
    "    # Remove rows and columns that would lead to df being singular\n",
    "    df = df.dropna(axis = 'columns') # drop columns containing missing values \n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    columnNames = list(df)\n",
    "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
    "        columnNames = columnNames[:10]\n",
    "    df = df[columnNames]\n",
    "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
    "    corrs = df.corr().values\n",
    "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
    "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
    "    plt.suptitle('Scatter and Density Plot')\n",
    "    plt.show()\n",
    "plotScatterMatrix(california_df, 30, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd2d79-4279-4f37-a83d-970caf9960ff",
   "metadata": {},
   "source": [
    "#### Scatter Plot Comments\n",
    "The scatter plot matrix reveals that 'MedInc' exhibits the strongest linear relationship with 'MedHouseVal'. While the relationship is noisy, the general upward trend suggests that higher median incomes are associated with higher house prices. Other features, such as 'AveRooms' and 'HouseAge', show weaker or non-linear relationships with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4363053-f527-4488-9716-00ba8c31e631",
   "metadata": {},
   "source": [
    "### 3. Model Implementation \n",
    "This project uses linear regression to model the relationship between median income and median house prices in California. Linear regression aims to find the line that best predicts the target variable (house price) based on the input feature (median income).\n",
    "\n",
    "#### 3.1 Linear Regression Model\n",
    "A linear model that predicts $f_{w,b}(x^{(i)})$ is defined as:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "\n",
    "Where:\n",
    "- $x^{(i)}$ is the median income for the (i)-th block group (the input feature).\n",
    "- $w$ is the weight or slope, representing the change in predicted house price for a one-unit change in median income.\n",
    "- $b$ is the bias or y-intercept, representing the predicted house price when the median income is zero.\n",
    "\n",
    "The goal of linear regression is to find the optimal values for the parameters $w$ and $b$ that minimize the difference between the model's predictions and the actual house prices.\n",
    "#### 3.2 The Cost Function (MSE)\n",
    "To measure the model's performance, we use the *Mean Squared Error (MSE) cost function*, $J(w,b)$. The MSE calculates the average of the squared differences between the predicted prices and the actual prices:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ \n",
    "Where:\n",
    "\n",
    "- $m$ is the number of training examples (block groups).\n",
    "- $y^{(i)}$ is the actual median house price for the (i)-th block group.\n",
    "\n",
    "The cost function quantifies how well the model is performing; a lower cost indicates better predictions.\n",
    "\n",
    "(In linear regression, we use input training data *(X_train)* to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples $x^{(i)},y^{(i)}$)\n",
    "\n",
    "#### 3.2 Gradient Descent Summary\n",
    "\n",
    "*Gradient descent* is an iterative optimization algorithm used to find the values of $w$ and $b$ that minimize the cost function $J(w,b)$. The algorithm works by repeatedly updating the parameters in the direction of the steepest descent of the cost function's gradient.\n",
    "\n",
    "The parameters $w$ and $b$ are updated simultaneously using the following equations:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "Where:\n",
    "\n",
    "- $\\alpha$ is the learning rate, a positive scalar that controls the step size of each update.\n",
    "- $\\frac{\\partial J(w,b)}{\\partial w}$ is the partial derivative of the cost function with respect to $w$.\n",
    "- $\\frac{\\partial J(w,b)}{\\partial b}$ is the partial derivative of the cost function with respect to $b$.\n",
    "Note: here *simultaniously* means that partial derivatives are calculated before updating any of the parameters.\n",
    "\n",
    "The partial derivatives are calculated as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "These equations calculate the gradient of the cost function with respect to $w$ and $b$, indicating the direction in which the cost function increases most rapidly. By subtracting a fraction $(\\alpha)$ of this gradient from the current parameter values, we move closer to the minimum of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69628ae7-416a-42ce-901b-0ee613f8e6d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = california_df[['MedInc']].values  # Independent variable (Median Income)       - feature\n",
    "y = california_df['MedHouseVal'].values  # Dependent variable (Median House Value) - target value\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13631555-a829-4f6a-9531-3d9e8d4df199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Difine the Cost Function: J(w,b)\n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the Cost Function J(w,b)\n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples\n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)     : model parameters\n",
    "    Returns\n",
    "      the Cost function J\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost # as defined in (2)\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "# Defining the Gradient as defined in (4) & (5)\n",
    "def compute_gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples\n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)     : model parameters\n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0 # initialise dj_dw\n",
    "    dj_db = 0 # initialise dj_db\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] # as defined in (4)\n",
    "        dj_db_i = f_wb - y[i]         # as defined in (5)\n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "# Defining the Gradient Descent Algorithm\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples\n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in,b_in (scalar): initial values of model parameters\n",
    "      alpha (float):       Learning rate\n",
    "      num_iters (int):     number of iterations to run gradient descent\n",
    "      cost_function:       function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "\n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b]\n",
    "    \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)       \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db\n",
    "        w = w - alpha * dj_dw       \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:       # prevent resource exhaustion\n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "\n",
    "    return w, b, J_history, p_history # return w and J,w history for graphing\n",
    "\n",
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "\n",
    "# Reshape X_train\n",
    "X_train = X_train.reshape(-1)\n",
    "\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(X_train ,y_train, w_init, b_init, tmp_alpha,\n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1774494b-275a-4db6-ad48-637c74c03d70",
   "metadata": {},
   "source": [
    "##### Cost Function Visualization:\n",
    "\n",
    "The plot below shows the value of the cost function, $J(w,b)$, at each iteration of the gradient descent algorithm. This visualization provides insights into:\n",
    "- **Convergence**: A decreasing cost function confirms that the gradient descent algorithm is working correctly and converging towards optimal parameter values.\n",
    "- **Learning Rate**: The plot helps assess the learning rate, $\\alpha$. A very slow decrease suggests that a larger $\\alpha$ might be needed, while fluctuations or increases indicate that  $\\alpha$ is too large.\n",
    "- **Iteration Count**: The plot aids in determining the required number of iterations. The algorithm should continue until the cost function reaches a plateau, indicating convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b8009-783e-418a-8988-7810b908f6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot cost versus iteration  - Cost should always decrease in successful runs\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100],c='m')\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:], c='purple')\n",
    "ax1.set_title(\"Cost vs. Iteration(start)\");  ax2.set_title(\"Cost vs. Iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('Iteration Step')  ;  ax2.set_xlabel('Iteration Step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332369e7-0572-4943-bd0d-c68210422352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = 0.4193\n",
    "b = 0.4446\n",
    "def compute_model_output(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the prediction of a linear model\n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      f_wb (ndarray (m,)): model prediction\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    f_wb = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        f_wb[i] = w * x[i] + b\n",
    "        \n",
    "    return f_wb\n",
    "tmp_f_wb = compute_model_output(X_train, w, b,)\n",
    "\n",
    "# Plot our model prediction\n",
    "plt.plot(X_train, tmp_f_wb, c='royalblue',label='Our Prediction')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_train, y_train, marker='x', c='lightseagreen',label='Actual Values')\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Housing Prices\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Median House Price (in units of $100,000)')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Median Income (in units of $100,000)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12ef7c-17ea-4bde-b887-79b5c9adea31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Calculate predictions on training and test sets\n",
    "train_predictions = compute_model_output(X_train, w_final, b_final)\n",
    "test_predictions = compute_model_output(X_test, w_final, b_final)\n",
    "\n",
    "\n",
    "# 2. Calculate MSE for training and test sets\n",
    "train_mse = compute_cost(X_train, y_train, w_final, b_final)\n",
    "X_test = X_test.reshape(-1) # reshaping X_test same as X_train\n",
    "test_mse = compute_cost(X_test, y_test, w_final, b_final)\n",
    "\n",
    "\n",
    "# 3. Print the MSE values\n",
    "print(f\"Training MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e7d45-82d6-4d75-89bf-dcf7206af0d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot our model prediction\n",
    "plt.plot(X_train, train_predictions, c='royalblue', label='Our Prediction')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_train, y_train, marker='x', c='lightseagreen', label='Actual Values')\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Housing Prices\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Median House Price (in units of $100,000)')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Median Income (in units of $100,000)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot cost versus iteration  - Cost should always decrease in successful runs\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist[:100], color = 'mediumorchid')\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:], color = 'mediumorchid')\n",
    "ax1.set_title(\"Cost vs. Iteration(start)\");\n",
    "ax2.set_title(\"Cost vs. Iteration (end)\")\n",
    "ax1.set_ylabel('Cost');\n",
    "ax2.set_ylabel('Cost')\n",
    "ax1.set_xlabel('Iteration Step');\n",
    "ax2.set_xlabel('Iteration Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3345626-86e4-430c-a2e4-d75d68238683",
   "metadata": {},
   "source": [
    "### 4. Conclusion \n",
    "The current linear regression model presented in this notebook, trained with only a single feature, provided a foundational understanding of the algorithm's mechanics. As anticipated, the predictions were limited to a simple straight line, which proved to be quite inaccurate and unable to capture the true complexity and often non-linear patterns observed in real-world housing data.\n",
    "\n",
    "This basic model serves primarily to illustrate the fundamental concept of linear regression. Its simplicity clearly highlights the necessity of providing more comprehensive and suitably prepared information to achieve meaningful predictive power.\n",
    "\n",
    "Therefore, the next steps in this analysis will involve:\n",
    "\n",
    "- Incorporating multiple features: To provide a richer context for predictions.\n",
    "\n",
    "- Applying feature scaling (normalization): To ensure efficient and stable convergence of the Gradient Descent algorithm.\n",
    "\n",
    "- Exploring initial feature engineering: To enable the linear model to capture more nuanced relationships beyond a single straight line.\n",
    "\n",
    "These crucial improvements will be detailed and analyzed in the subsequent notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
