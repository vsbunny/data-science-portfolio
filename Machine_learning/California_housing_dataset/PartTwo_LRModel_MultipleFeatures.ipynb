{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d342943a-c96c-4313-a3e8-670622183f16",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent (California Housing Dataset) - Vectorization and Multiple Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5b266-4479-49d2-83ed-078146f904de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, math\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f65e97-3c76-46e1-8fa6-8f367e11aab2",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This notebook implements a multivariate Linear Regression model using vectorized operations for efficiency. It covers data loading, initial feature selection, feature scaling (normalization), vectorized computation of cost and gradient, and the Gradient Descent algorithm for parameter optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf209192-af3f-49a4-915c-cf065e4da0af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Data Loading and Initial Preparation ---\n",
    "# Load the California Housing dataset. This dataset is suitable for regression tasks.\n",
    "housing = fetch_california_housing()\n",
    "# Create a Pandas DataFrame from the features, using the feature names for columns.\n",
    "california_df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "# Add the target variable (Median House Value) to the DataFrame.\n",
    "california_df['MedHouseVal'] = housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e2484-144b-46f9-abe5-18bc90e2a010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "california_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a225f-5ee9-4de6-9376-26ed27ab7962",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "The California Housing dataset contains information about housing prices in California districts. Each row represents a census block group, and the columns describe various attributes of that region. We will explore the relationship between the features and the target variable, Median House Value ('MedHouseVal'), to determine which features are most relevant for predicting house prices.\n",
    "\n",
    "The 'MedInc' feature represents the median income in each block group, measured in tens of thousands of dollars. We expect that 'MedInc' will have a positive correlation with 'MedHouseVal', as areas with higher median incomes typically have higher property values.\n",
    "\n",
    "To understand the relationship between the features and the target variable (MedHouseVal), we use a scatter plot matrix. This helps us identify potential linear relationships that are suitable for linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a3146-a4d5-4466-8847-c1e391b4ad60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the initial set of features to be used for the linear regression model.\n",
    "# These features are chosen to represent key factors influencing house prices.\n",
    "selected_features = [\n",
    "    'MedInc',      # Median Income in block group\n",
    "    'HouseAge',    # Median house age in block group\n",
    "    'AveRooms',    # Average number of rooms per household\n",
    "    'Population'   # Block group population\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9b51f-a4ed-4010-a208-58e50e3fe548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the feature matrix (X_train) and target vector (y_train) as NumPy arrays.\n",
    "# .values converts the Pandas DataFrame/Series to a NumPy array, which is essential\n",
    "# for vectorized operations.\n",
    "X_train = california_df[selected_features].values # Array containing the selected features\n",
    "y_train = california_df['MedHouseVal'].values      # The target variable\n",
    "\n",
    "#print(f\"X_train shape (initial features): {X_train.shape}\")\n",
    "#print(f\"y_train shape: {y_train.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36253ae8-c12a-48d7-9820-e7e0c146814e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Scatter plot depicting relationships between the variables ---\n",
    "# X_features should be the list of names that correspond to the columns in X_train\n",
    "X_features = selected_features\n",
    "\n",
    "fig,ax=plt.subplots(1, 4, figsize=(16, 4), sharey=True)  \n",
    "for i in range(len(ax)):\n",
    "    # Plot each feature against the target\n",
    "    ax[i].scatter(X_train[:,i], y_train, s=5, alpha=0.5, color = 'royalblue') # Added 's' for marker size, 'alpha' for transparency\n",
    "    ax[i].set_xlabel(X_features[i])                      # Set x-axis label using the feature name\n",
    "\n",
    "ax[0].set_ylabel(\"Median House Value\")\n",
    "plt.tight_layout() # Adjust subplot parameters for a tight layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e21a1-09a7-4640-811a-fa81c6e1cdb4",
   "metadata": {},
   "source": [
    "## 3. Normalization (Z-score normalization)\n",
    "Feature scaling (specifically Z-score normalization or Standardization) is critical for Gradient Descent to converge efficiently and effectively, especially when features have different scales, units, or ranges. It helps prevent features with larger values from dominating the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507ef36-9fea-478c-ba94-a76110016fac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalization \n",
    "# Calculate the mean (mu) of each feature (column-wise, axis=0).\n",
    "mu     = np.mean(X_train,axis=0)\n",
    "# Calculate the standard deviation (sigma) of each feature (column-wise, axis=0).\n",
    "sigma  = np.std(X_train,axis=0)\n",
    "# Handle potential division by zero: if a feature has zero standard deviation (all values are identical),\n",
    "# replace 0 with a very small number to prevent errors during division.\n",
    "sigma[sigma == 0] = 1e-10\n",
    "\n",
    "# Apply Z-score normalization: (x - mean) / standard_deviation.\n",
    "# This transforms each feature to have a mean of 0 and a standard deviation of 1.\n",
    "X_norm = (X_train - mu)/sigma\n",
    "\n",
    "# X_mean is an intermediate step for visualization, showing data centered around zero\n",
    "# before scaling by standard deviation.\n",
    "X_mean = (X_train - mu)\n",
    "\n",
    "print(f\"X_norm shape after normalization: {X_norm.shape}\")\n",
    "\n",
    "# --- Normalization Plot ---\n",
    "# This section visualizes the effect of Z-score normalization on the distribution\n",
    "# of two selected features. It helps confirm that the data is being scaled\n",
    "# appropriately for gradient descent.\n",
    "#print(\"\\n3. Visualizing Feature Normalization...\")\n",
    "\n",
    "# Plotting the relationship between the first and fourth features (MedInc vs Population)\n",
    "# in their original, mean-centered, and Z-score normalized states.\n",
    "fig,ax=plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "# Subplot 1: Unnormalized data\n",
    "ax[0].scatter(X_train[:,0], X_train[:,3], color = 'royalblue')\n",
    "ax[0].set_xlabel(selected_features[0]); ax[0].set_ylabel(selected_features[3]);\n",
    "ax[0].set_title(\"Unnormalized\")\n",
    "ax[0].axis('equal') # Ensures equal aspect ratio for visual comparison\n",
    "\n",
    "# Subplot 2: Mean-centered data (X - mu)\n",
    "ax[1].scatter(X_mean[:,0], X_mean[:,3], color = 'royalblue')\n",
    "ax[1].set_xlabel(selected_features[0]); ax[1].set_ylabel(selected_features[3]);\n",
    "ax[1].set_title(r\"X - $\\mu$ (Mean Centered)\")\n",
    "ax[1].axis('equal')\n",
    "\n",
    "# Subplot 3: Z-score normalized data\n",
    "ax[2].scatter(X_norm[:,0], X_norm[:,3], color = 'mediumorchid')\n",
    "ax[2].set_xlabel(selected_features[0]); ax[2].set_ylabel(selected_features[3]);\n",
    "ax[2].set_title(r\"Z-score Normalized\")\n",
    "ax[2].axis('equal')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjusts subplot parameters for a tight layout with a main title\n",
    "fig.suptitle(\"Distribution of Features: Before, During, and After Normalization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef633c43-16c4-4e79-93c5-76da5434ddc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Vectorized Cost Function ---\n",
    "# This function calculates the Mean Squared Error (MSE) cost, a common metric for\n",
    "# regression problems. It uses vectorized NumPy operations for high efficiency.\n",
    "def compute_cost_vectorized(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost (Mean Squared Error) for linear regression using a vectorized approach.\n",
    "    This function calculates the cost for all examples simultaneously, leveraging NumPy's\n",
    "    efficient array operations, which is significantly faster than using explicit loops.\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, where m is the number of examples and n is the number of features.\n",
    "      y (ndarray (m,)) : Target values corresponding to each example.\n",
    "      w (ndarray (n,)) : Model parameters (weights) for each feature.\n",
    "      b (scalar)       : Model parameter (bias term/y-intercept).\n",
    "\n",
    "    Returns:\n",
    "      cost (scalar): The computed cost (J) value.\n",
    "    \"\"\"\n",
    "    m = X.shape[0] # Number of training examples\n",
    "\n",
    "    # Calculate predictions (f_wb) for all examples simultaneously.\n",
    "    # X (m,n) @ w (n,) performs matrix-vector multiplication, resulting in a vector of shape (m,).\n",
    "    # 'b' (scalar) is then broadcast across this entire vector.\n",
    "    f_wb = X @ w + b\n",
    "\n",
    "    # Calculate the squared error for all examples.\n",
    "    # (f_wb - y) results in a vector of errors (shape m,), then element-wise squaring.\n",
    "    squared_error = (f_wb - y)**2\n",
    "\n",
    "    # Sum all squared errors and apply the cost function formula (1/2m * sum(error^2)).\n",
    "    cost = np.sum(squared_error) / (2 * m)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "# --- Vectorized Gradient Function ---\n",
    "# This function calculates the gradients of the cost function with respect to\n",
    "# the model parameters (weights 'w' and bias 'b'). These gradients indicate\n",
    "# the direction and magnitude of the steepest ascent of the cost function,\n",
    "# which Gradient Descent uses to move towards the minimum.\n",
    "def compute_gradient_vectorized(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the cost function with respect to the parameters w and b,\n",
    "    using a vectorized approach. This method is much more efficient than using nested loops,\n",
    "    especially for datasets with many examples or features.\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, where m is the number of examples and n is the number of features.\n",
    "      y (ndarray (m,)) : Target values corresponding to each example.\n",
    "      w (ndarray (n,)) : Model parameters (weights) for each feature.\n",
    "      b (scalar)       : Model parameter (bias term/y-intercept).\n",
    "\n",
    "    Returns:\n",
    "      dj_db (scalar):      The gradient of the cost with respect to the bias term 'b'.\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost with respect to the weights 'w'.\n",
    "    \"\"\"\n",
    "    m, n = X.shape # Get number of training examples (m) and features (n)\n",
    "\n",
    "    # Calculate predictions (f_wb) for all examples.\n",
    "    f_wb = X @ w + b\n",
    "\n",
    "    # Calculate the error for all examples (prediction - actual target).\n",
    "    # This results in a vector of errors (shape m,).\n",
    "    err = f_wb - y\n",
    "\n",
    "    # Calculate dj_db (gradient with respect to b).\n",
    "    # This is the sum of all errors, divided by m.\n",
    "    dj_db = np.sum(err) / m\n",
    "\n",
    "    # Calculate dj_dw (gradient with respect to w).\n",
    "    # This is achieved by multiplying the transpose of the feature matrix (X.T)\n",
    "    # with the error vector, then dividing by m.\n",
    "    # X.T (n,m) @ err (m,) results in a vector of shape (n,),\n",
    "    # which is the correct shape for dj_dw (one gradient component per feature).\n",
    "    dj_dw = (X.T @ err) / m\n",
    "\n",
    "    return dj_db, dj_dw\n",
    "\n",
    "\n",
    "# --- Gradient Descent Function ---\n",
    "# This function implements the batch Gradient Descent algorithm, an iterative optimization\n",
    "# algorithm used to find the set of model parameters (w and b) that minimize the cost function.\n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn optimal parameters w and b.\n",
    "    It updates w and b by taking 'num_iters' steps down the gradient,\n",
    "    using the specified learning rate 'alpha'.\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features.\n",
    "      y (ndarray (m,)) : Target values.\n",
    "      w_in (ndarray (n,)): Initial model parameters (weights).\n",
    "      b_in (scalar)      : Initial model parameter (bias term).\n",
    "      cost_function      : A function (e.g., compute_cost_vectorized) to compute the cost.\n",
    "      gradient_function  : A function (e.g., compute_gradient_vectorized) to compute the gradient.\n",
    "      alpha (float)      : Learning rate, controls the size of the steps taken down the gradient.\n",
    "      num_iters (int)    : Number of iterations (gradient steps) to run.\n",
    "\n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated (learned) values of the model parameters (weights).\n",
    "      b (scalar)       : Updated (learned) value of the model parameter (bias term).\n",
    "      J_history (list):  A list containing the cost (J) at each iteration,\n",
    "                         useful for monitoring convergence.\n",
    "    \"\"\"\n",
    "    # J_history stores the cost at each iteration, used for plotting convergence.\n",
    "    J_history = []\n",
    "    # Create deep copies of initial parameters to avoid modifying global variables.\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient (dj_db, dj_dw) for the current w and b using the provided gradient function.\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)\n",
    "\n",
    "        # Update Parameters (w and b) using the gradient descent update rule:\n",
    "        # parameter = parameter - learning_rate * gradient_of_parameter\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        # Save the cost J at each iteration. This conditional prevents excessive memory use\n",
    "        # if 'num_iters' is very large, though for 1000 iterations it's not strictly necessary.\n",
    "        if i < 100000:\n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "\n",
    "        # Print the cost every 1/10th of the total iterations to monitor progress.\n",
    "        # math.ceil ensures at least one print if num_iters is very small.\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}\")\n",
    "\n",
    "    return w, b, J_history # Return the final learned parameters and the cost history\n",
    "\n",
    "\n",
    "# --- Initialize Parameters for Gradient Descent ---\n",
    "n_features = X_norm.shape[1] # Get the number of features from the normalized data\n",
    "w_in = np.zeros(n_features)  # w must be an array of zeros with 'n_features' elements\n",
    "b_in = 0\n",
    "\n",
    "# --- Run Gradient Descent with Vectorized Functions ---\n",
    "# Use the vectorized versions of the cost and gradient functions\n",
    "w_norm, b_norm, hist = gradient_descent(X_norm, y_train, w_in, b_in,\n",
    "                                        compute_cost_vectorized,\n",
    "                                        compute_gradient_vectorized,\n",
    "                                        1.0e-1, 1000)\n",
    "\n",
    "print(f\"\\nFinal w found by gradient descent: {w_norm}\")\n",
    "print(f\"Final b found by gradient descent: {b_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ced118-e5df-4999-931d-6b710265f61d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Plot cost versus iteration - Cost should always decrease in successful runs ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "\n",
    "# Plot the beginning (first 100 iterations, indices 0-99)\n",
    "ax1.plot(hist[:100], color = 'mediumorchid')\n",
    "ax1.set_title(\"Cost vs. Iteration (Start)\");\n",
    "ax1.set_ylabel('Cost');\n",
    "ax1.set_xlabel('Iteration Step');\n",
    "\n",
    "# Plot the rest of the iterations (from iteration 100 to the end)\n",
    "# The data for ax2 starts from index 100 of 'hist'\n",
    "start_index_for_end_plot = 100\n",
    "ax2.plot(start_index_for_end_plot + np.arange(len(hist[start_index_for_end_plot:])), hist[start_index_for_end_plot:], color = 'mediumorchid')\n",
    "ax2.set_title(\"Cost vs. Iteration (End)\")\n",
    "ax2.set_ylabel('Cost')\n",
    "ax2.set_xlabel('Iteration Step')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bade29f-0589-47f6-9bd1-2205ddaa4e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Target Prediction ---\n",
    "# Define custom colors for this plot\n",
    "TARGET_COLOR_LR_INITIAL = 'cadetblue' # A muted blue/gray for target points\n",
    "PREDICT_COLOR_LR_INITIAL = 'lightcoral' # A soft red/coral for prediction points\n",
    "\n",
    "m = X_norm.shape[0] # Get the number of examples in the normalized dataset (X_norm)\n",
    "yp = np.zeros(m)    # Initialize an array to store the predicted target values (yp = y_predicted)\n",
    "\n",
    "# Iterate through each training example to calculate its prediction individually.\n",
    "# This loop performs the dot product of the example's features with the learned weights (w_norm)\n",
    "# and adds the bias term (b_norm) for each row.\n",
    "for i in range(m):\n",
    "    yp[i] = np.dot(X_norm[i], w_norm) + b_norm\n",
    "\n",
    "# Create a scatter plot showing the target vs prediction\n",
    "fig,ax=plt.subplots(1,4,figsize=(16, 4),sharey=True) # Create a figure with 4 subplots (1 row, 4 columns)\n",
    "                                                  # sharey=True ensures all subplots share the same y-axis scale.\n",
    "\n",
    "# Loop through each of the selected features to create a scatter plot in its respective subplot.\n",
    "for i in range(len(ax)):\n",
    "    # Plot the actual target values (y_train) against the original feature data (X_train[:,i]).\n",
    "    ax[i].scatter(X_train[:,i],y_train,\n",
    "                  label = 'Target',     # Label for the legend\n",
    "                  s=5,                  # Marker size\n",
    "                  alpha=0.5,            # Transparency (0.5 makes overlapping points darker)\n",
    "                  color=TARGET_COLOR_LR_INITIAL) # Custom color for target points\n",
    "\n",
    "    ax[i].set_xlabel(X_features[i]) # Set the x-axis label using the corresponding feature name.\n",
    "\n",
    "    # Plot the model's predictions (yp) against the same original feature data (X_train[:,i]).\n",
    "    ax[i].scatter(X_train[:,i],yp,\n",
    "                  label = 'Predict',    # Label for the legend\n",
    "                  s=5,                  # Marker size\n",
    "                  alpha=0.7,            # Slightly less transparent to stand out\n",
    "                  color=PREDICT_COLOR_LR_INITIAL) # Custom color for prediction points\n",
    "\n",
    "    ax[i].set_title(X_features[i]) # Set the subplot title to the name of the feature being plotted.\n",
    "\n",
    "ax[0].set_ylabel(\"Median House Value\"); # Set the common y-axis label for the first subplot.\n",
    "ax[0].legend(); # Display the legend on the first subplot (it applies to all due to sharey=True).\n",
    "fig.suptitle(\"Target vs. Prediction: Initial Linear Regression Model (on original features)\", y=1.02) # Main title for the entire figure.\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust subplot parameters for a tight layout, leaving space for the suptitle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf927c-b6ac-4ab7-a5b7-82aa73facc12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Vectorized Prediction --- \n",
    "# Predict target values (yp) using the normalized features (X_norm),\n",
    "# the learned weights (w_norm), and the bias term (b_norm).\n",
    "# X_norm (m,n) @ w_norm (n,) performs matrix-vector multiplication resulting in a (m,) vector.\n",
    "# 'b_norm' (scalar) is broadcast across this vector.\n",
    "yp = X_norm @ w_norm + b_norm # Fully vectorized prediction!\n",
    "\n",
    "print(f\"Predictions generated for {len(yp)} examples using vectorized approach.\")\n",
    "\n",
    "\n",
    "# --- Plot predictions and targets versus original features (Vectorized LR Model) ---\n",
    "# This plot visually assesses the performance of the trained Linear Regression model after\n",
    "# vectorization. It compares the model's predicted values ('yp') against the actual target\n",
    "# values ('y_train') for each of the original features. Plotting against X_train (original features)\n",
    "# helps in interpreting the model's fit on the natural scale of the data.\n",
    "#print(\"Plotting Vectorized Linear Regression predictions...\")\n",
    "\n",
    "# Define custom colors for this plot. These colors aim to provide clear distinction\n",
    "# between actuals and predictions, and can be compared to previous plots.\n",
    "TARGET_COLOR_LR_VECTORIZED = 'cadetblue' # Consistent blue/gray for target points\n",
    "PREDICT_COLOR_LR_VECTORIZED = 'goldenrod' # A distinct, warm color for predictions\n",
    "\n",
    "fig,ax=plt.subplots(1,4,figsize=(16, 4),sharey=True) # Create a figure with 4 subplots\n",
    "\n",
    "# Loop through each of the selected features to generate a scatter plot in its subplot.\n",
    "for i in range(len(ax)):\n",
    "    # Plot the actual target values (y_train) against the original feature data.\n",
    "    ax[i].scatter(X_train[:,i],y_train,\n",
    "                  label = 'Target',\n",
    "                  s=5,\n",
    "                  alpha=0.5,\n",
    "                  color=TARGET_COLOR_LR_VECTORIZED)\n",
    "\n",
    "    ax[i].set_xlabel(X_features[i]) # Set x-axis label.\n",
    "\n",
    "    # Plot the model's predictions (yp) against the same original feature data.\n",
    "    ax[i].scatter(X_train[:,i],yp,\n",
    "                  label = 'Predict',\n",
    "                  s=5,\n",
    "                  alpha=0.7,\n",
    "                  color=PREDICT_COLOR_LR_VECTORIZED) # Apply custom color for predictions\n",
    "    ax[i].set_title(X_features[i]) # Set subplot title.\n",
    "\n",
    "ax[0].set_ylabel(\"Median House Value\"); # Set common y-axis label for the first subplot.\n",
    "ax[0].legend(); # Display legend on the first subplot.\n",
    "fig.suptitle(\"Target vs. Prediction: Vectorized Linear Regression Model (on original features)\", y=1.02) # Main title for the figure.\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout for better spacing.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d43a68-9ec1-49f7-897e-9fabdefd8ce2",
   "metadata": {},
   "source": [
    "### 3.1 Observation: \n",
    "While vectorization significantly improved the *computational efficiency* of the linear regression model, the visual predictions (especially for 'MedInc') still show clear limitations. The linear model inherently struggles to capture highly non-linear relationships and varying data dispersion (heteroscedasticity), often resulting in a somewhat narrow prediction band that doesn't fully cover the true spread of target values. This indicates that a simple linear model, even with optimized implementation, might not be the best fit for all underlying patterns in the dataset.\n",
    "\n",
    "### Next Steps: \n",
    "To address these remaining limitations and further improve predictive accuracy,\n",
    "the analysis will proceed with:\n",
    "- 1. Further Feature Engineering: By adding polynomial transformations, logarithmic transforms,\n",
    "   and interaction terms, we aim to allow the linear model to better approximate complex\n",
    "   relationships by transforming the input features.\n",
    "- 2. Exploring More Flexible Models: We will then investigate the performance of non-linear\n",
    "   models like Random Forest Regressors, which can inherently handle such complexities\n",
    "   and feature interactions without explicit manual engineering for every non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1931a-8535-4692-8997-ce93738a6f32",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering - Improving the model by adding a polynomial feature \n",
    "\n",
    "The initial Linear Regression model, while efficient due to vectorization, revealed some limitations in its ability to capture all underlying patterns in the data. Specifically, the visual analysis of predictions (e.g., for 'MedInc') showed that the linear model struggled with non-linear relationships and varying data dispersion.\n",
    "\n",
    "To overcome these limitations and enable the inherently linear model to capture more complex patterns, we employ Feature Engineering. This crucial process involves transforming existing features or creating new ones from the raw data. By doing so, we can:\n",
    "\n",
    "- Approximate Non-Linearity: Introduce polynomial terms (e.g., $x^{2}$) to allow the linear model to fit curves.\n",
    "- Handle Skewed Distributions: Apply transformations like logarithmic scaling (e.g., $log(x)$) to make skewed features more symmetrical and their relationships more linear-like.\n",
    "- Capture Interactions: Create new features by combining existing ones (e.g., multiplying two features) to model how the effect of one feature might depend on another.\n",
    "\n",
    "Through this strategic feature engineering, the linear regression model will be provided with more informative inputs, allowing it to achieve a better fit to the underlying data patterns without changing the model's core linear assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebc005-3353-45ce-8e43-e5642109bb35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Redefine X_train and y_train for Model V2\n",
    "# We re-initialize the feature matrix (X_train_v2) and target (y_train_v2)\n",
    "# for this iteration. This ensures a clean slate for feature engineering for this model.\n",
    "# (Assumes 'california_df' is loaded from the start of the notebook)\n",
    "initial_feature_names = [\n",
    "    'MedInc',\n",
    "    'HouseAge',\n",
    "    'AveRooms',\n",
    "    'Population'\n",
    "]\n",
    "X_train_v2 = california_df[initial_feature_names].values # Create new feature matrix for V2\n",
    "y_train_v2 = california_df['MedHouseVal'].values         # Target remains the same\n",
    "X_features_v2 = list(initial_feature_names)              # Mutable list to track features for V2 model\n",
    "\n",
    "print(f\"\\nInitial X_train_v2 shape: {X_train_v2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf99300-9ea1-4b3b-a906-e1addc5a4c21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Add Polynomial Feature for 'MedInc' to X_train_v2\n",
    "print(\"Adding 'MedInc_Sq' (Median Income squared) feature...\")\n",
    "# Rationale: This polynomial term aims to help the linear model better capture\n",
    "# the observed non-linear (curved) relationship of 'MedInc' with the target.\n",
    "medinc_original_column_index_v2 = X_features_v2.index('MedInc')\n",
    "medinc_column_data_v2 = X_train_v2[:, medinc_original_column_index_v2]\n",
    "medinc_squared_data_v2 = medinc_column_data_v2**2\n",
    "\n",
    "X_features_v2.append('MedInc_Sq') # Add the new feature's name to our tracking list\n",
    "X_train_v2 = np.c_[X_train_v2, medinc_squared_data_v2] # Concatenate the new squared feature as a new column\n",
    "\n",
    "print(f\"X_train_v2 shape after adding 'MedInc_Sq': {X_train_v2.shape}\")\n",
    "print(f\"Updated feature list for Model V2: {X_features_v2}\")\n",
    "\n",
    "\n",
    "# 3. Normalize the NEW X_train_v2 (with the added feature)\n",
    "# Normalization (Z-score scaling) is performed again on the expanded feature set.\n",
    "# This is crucial because adding new features changes the overall scale and distribution,\n",
    "# and Gradient Descent requires features to be on a similar scale for efficient convergence.\n",
    "print(\"\\nNormalizing features for Model V2 (Z-score normalization)...\")\n",
    "mu_v2     = np.mean(X_train_v2, axis=0)  # Calculate mean for each feature in the new X_train_v2\n",
    "sigma_v2  = np.std(X_train_v2, axis=0)   # Calculate standard deviation for each feature\n",
    "sigma_v2[sigma_v2 == 0] = 1e-10          # Handle potential division by zero\n",
    "\n",
    "X_norm_v2 = (X_train_v2 - mu_v2) / sigma_v2 # Apply Z-score normalization\n",
    "\n",
    "print(f\"X_norm_v2 shape after normalization: {X_norm_v2.shape}\")\n",
    "\n",
    "\n",
    "# 4. Run Gradient Descent with the new X_norm_v2\n",
    "# The Gradient Descent algorithm is now executed with the expanded and normalized\n",
    "# feature set (X_norm_v2). This trains \"Model V2\" to find optimal weights (w_final_v2)\n",
    "# and bias (b_final_v2) that incorporate the new polynomial feature.\n",
    "print(\"\\nRunning Gradient Descent for Model V2...\")\n",
    "n_features_v2 = X_norm_v2.shape[1] # Get the total number of features (now 5)\n",
    "w_in_v2 = np.zeros(n_features_v2)  # Initialize weights as zeros for the 5 features\n",
    "b_in_v2 = 0                        # Initialize bias as zero\n",
    "\n",
    "alpha_v2 = 1.0e-1 # Learning rate\n",
    "num_iters_v2 = 1000 # Number of iterations\n",
    "\n",
    "# Call the gradient_descent function using the updated data and initial parameters.\n",
    "# (compute_cost_vectorized and compute_gradient_vectorized functions are defined earlier)\n",
    "w_final_v2, b_final_v2, hist_v2 = gradient_descent(X_norm_v2, y_train_v2, w_in_v2, b_in_v2,\n",
    "                                                compute_cost_vectorized,\n",
    "                                                compute_gradient_vectorized,\n",
    "                                                alpha_v2, num_iters_v2)\n",
    "\n",
    "print(f\"\\nFinal w for Model V2: {w_final_v2}\")\n",
    "print(f\"Final b for Model V2: {b_final_v2}\")\n",
    "\n",
    "\n",
    "# 5. Plot Cost History for Model V2\n",
    "# This plot visualizes the convergence of the cost function for Model V2 over iterations.\n",
    "# A continuously decreasing cost indicates successful learning.\n",
    "print(\"\\nPlotting Cost vs. Iteration for Model V2...\")\n",
    "fig, (ax1_v2, ax2_v2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1_v2.plot(hist_v2[:100], color = 'mediumorchid') # Plot first 100 iterations (start of convergence)\n",
    "ax1_v2.set_title(\"Cost vs. Iteration (Model V2 - Start)\");\n",
    "ax1_v2.set_ylabel('Cost');\n",
    "ax1_v2.set_xlabel('Iteration Step');\n",
    "start_index_for_end_plot_v2 = 100\n",
    "ax2_v2.plot(start_index_for_end_plot_v2 + np.arange(len(hist_v2[start_index_for_end_plot_v2:])), hist_v2[start_index_for_end_plot_v2:], color = 'mediumorchid')\n",
    "ax2_v2.set_title(\"Cost vs. Iteration (Model V2 - End)\") # Plot remaining iterations (end of convergence)\n",
    "ax2_v2.set_ylabel('Cost')\n",
    "ax2_v2.set_xlabel('Iteration Step')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 6. Predict and Plot with Final Model V2\n",
    "# This section generates predictions using the trained Model V2 and visualizes its performance\n",
    "# against the original features. This helps assess if the added polynomial feature improved the fit.\n",
    "\n",
    "# Make predictions using the optimized Model V2 parameters (w_final_v2, b_final_v2)\n",
    "# and the normalized feature set (X_norm_v2).\n",
    "#print(\"\\nGenerating predictions and plotting for Model V2...\")\n",
    "\n",
    "yp_final_v2 = X_norm_v2 @ w_final_v2 + b_final_v2 # Vectorized prediction for Model V2\n",
    "\n",
    "# Define custom colors for Model V2 plots. These are distinct from initial LR plots\n",
    "# and prepare for comparison with Random Forest.\n",
    "TARGET_COLOR_V2 = 'cadetblue'         # Consistent target color\n",
    "PREDICT_COLOR_V2 = 'mediumorchid'     # New color for Model V2 predictions (suggests improvement)\n",
    "\n",
    "# Plotting the initial 4 features vs predictions from Model V2.\n",
    "# We use 'initial_feature_names' for the x-axis to maintain visual consistency\n",
    "# with previous plots, allowing for direct comparison of prediction patterns.\n",
    "fig,ax=plt.subplots(1,len(initial_feature_names),figsize=(16, 4),sharey=True)\n",
    "for i in range(len(initial_feature_names)):\n",
    "    ax[i].scatter(X_train_v2[:,i],y_train_v2,\n",
    "                  label = 'Target', s=5, alpha=0.5, color=TARGET_COLOR_V2) # Apply target color\n",
    "    ax[i].scatter(X_train_v2[:,i],yp_final_v2,\n",
    "                  label = 'Predict (V2)', s=5, alpha=0.7, color=PREDICT_COLOR_V2) # Apply prediction color\n",
    "    ax[i].set_xlabel(initial_feature_names[i])\n",
    "    ax[i].set_title(initial_feature_names[i])\n",
    "\n",
    "ax[0].set_ylabel(\"Median House Value\");\n",
    "ax[0].legend();\n",
    "fig.suptitle(\"Target vs. Prediction: Model V2 (incl. MedInc_Sq feature)\", y=1.02)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# --- Additional Plot: Explicitly plot MedInc (original) vs. predictions for Model V2 ---\n",
    "# This plot is crucial to visually confirm if the added polynomial term helped capture\n",
    "# the non-linear relationship in 'MedInc' more effectively than the initial linear model.\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_train_v2[:, medinc_original_column_index_v2], y_train_v2,\n",
    "            label='Target', s=5, alpha=0.5, color=TARGET_COLOR_V2)\n",
    "plt.scatter(X_train_v2[:, medinc_original_column_index_v2], yp_final_v2,\n",
    "            label='Predict (V2 with MedInc_Sq)', s=5, alpha=0.7, color=PREDICT_COLOR_V2)\n",
    "plt.xlabel(\"MedInc (Original)\")\n",
    "plt.ylabel(\"Median House Value\")\n",
    "plt.title(f\"MedInc (Original) vs. Target/Prediction (Model V2)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- End of Iteration 2 Analysis ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdd22c-5d04-46bb-9481-bc26aea3981b",
   "metadata": {},
   "source": [
    "## 5. Further Feature Engineering - Log Transformation\n",
    "\n",
    "Building on the previous efforts to capture non-linearity with polynomial terms, we now introduce **logarithmic transformations**. This is a particularly powerful technique for handling numerical features with specific distributional characteristics:\n",
    "\n",
    "- Addressing Skewed Distributions: Many real-world features, such as income, population counts, or asset values, often exhibit highly skewed distributions (e.g., a long tail to the right, where most values are concentrated at the lower end but a few outliers are extremely large). Linear models generally perform better when features are more symmetrically distributed (closer to a Gaussian or normal distribution). A logarithmic transformation effectively compresses the larger values and stretches out the smaller values, thereby making the distribution more symmetrical and manageable for the model.\n",
    "\n",
    "- Capturing Diminishing Returns: Logarithmic transforms are also excellent for modeling relationships where the impact of a feature on the target variable shows diminishing returns. For instance, an initial increase in median income might have a substantial impact on house value, but successive equivalent increases at already very high income levels might lead to a relatively smaller additional effect. A linear model applied to a logarithmically transformed feature can effectively capture this type of non-linear relationship.\n",
    "\n",
    "- Converting Multiplicative to Additive Relationships: In some cases, a multiplicative relationship between a feature and the target can become additive after a log transformation, making it more amenable to a linear model.\n",
    "\n",
    "In this analysis, features like 'Population' are often prime candidates for a logarithmic transformation due to their typically skewed distributions. We frequently use np.log1p(x) (which calculates $log(1+x)$) as it's a robust alternative to np.log(x), safely handling cases where feature values might be zero and preventing errors (as $log(0)$ is undefined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600efe8-0bc3-4ac0-b550-4abbe386e6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Re-initialize X_train with original 4 features (as we did for Model V2)\n",
    "# Start with the base set of features to ensure a clean and controlled\n",
    "# application of new feature engineering steps for this iteration (Model V3).\n",
    "# (Assumes 'california_df' is still loaded from the start of the notebook)\n",
    "initial_feature_names = [\n",
    "    'MedInc',\n",
    "    'HouseAge',\n",
    "    'AveRooms',\n",
    "    'Population'\n",
    "]\n",
    "X_train_v3 = california_df[initial_feature_names].values # Create a new feature matrix for Model V3\n",
    "y_train_v3 = california_df['MedHouseVal'].values         # Target variable remains unchanged\n",
    "X_features_v3 = list(initial_feature_names)              # Mutable list to track all features for Model V3\n",
    "\n",
    "print(f\"\\nInitial X_train_v3 shape: {X_train_v3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0d4c9-9a23-4124-888f-4adec207a0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Add ALL desired Polynomial, Log, and Interaction Features to X_train_v3\n",
    "print(\"Adding 'MedInc_Sq', 'Log_Population', and 'MedInc_x_AveRooms' features...\")\n",
    "\n",
    "# --- Feature 2.1: Add MedInc_Sq (Polynomial Feature) ---\n",
    "# Re-adding the squared term for 'MedInc' to continue capturing its non-linear trend.\n",
    "medinc_original_column_index_v3 = X_features_v3.index('MedInc') # Get original index of MedInc\n",
    "medinc_column_data_v3 = X_train_v3[:, medinc_original_column_index_v3]\n",
    "medinc_squared_data_v3 = medinc_column_data_v3**2\n",
    "X_features_v3.append('MedInc_Sq') # Add feature name\n",
    "X_train_v3 = np.c_[X_train_v3, medinc_squared_data_v3] # Concatenate to X_train_v3\n",
    "\n",
    "\n",
    "# --- Feature 2.2: Add Log_Population (Logarithmic Transformation) ---\n",
    "# Rationale: 'Population' often has a highly skewed distribution. A logarithmic\n",
    "# transformation helps to normalize skewed data, making its relationship with the\n",
    "# target more linear-like and improving model performance. Using `np.log1p` (log(1+x))\n",
    "# is robust as it handles zero values gracefully.\n",
    "population_original_column_index_v3 = initial_feature_names.index('Population') # Get original index from base features\n",
    "population_column_data_v3 = california_df['Population'].values # Use original DataFrame column for robustness\n",
    "log_population_data_v3 = np.log1p(population_column_data_v3)\n",
    "X_features_v3.append('Log_Population') # Add feature name\n",
    "X_train_v3 = np.c_[X_train_v3, log_population_data_v3] # Concatenate to X_train_v3\n",
    "\n",
    "\n",
    "# --- Feature 2.3: Add MedInc_x_AveRooms (Interaction Term) ---\n",
    "# Rationale: This feature captures the idea that the effect of 'AveRooms' on house value\n",
    "# might depend on the 'MedInc' level, or vice-versa. It allows the model to learn\n",
    "# a more nuanced relationship than if these features were considered only independently.\n",
    "# It's crucial to use the original, untransformed features for interaction terms.\n",
    "medinc_x_averooms_data_v3 = california_df['MedInc'].values * california_df['AveRooms'].values\n",
    "X_features_v3.append('MedInc_x_AveRooms') # Add feature name\n",
    "X_train_v3 = np.c_[X_train_v3, medinc_x_averooms_data_v3] # Concatenate to X_train_v3\n",
    "\n",
    "print(f\"X_train_v3 shape after adding all new features: {X_train_v3.shape}\")\n",
    "print(f\"Updated feature list for Model V3: {X_features_v3}\")\n",
    "\n",
    "\n",
    "# 3. Normalize the NEW X_train_v3 (with all added features)\n",
    "# After adding new features, it's essential to re-normalize the entire feature matrix.\n",
    "# This ensures that all features (original and engineered) are on a consistent scale\n",
    "# (mean 0, standard deviation 1), which is vital for the efficient convergence of Gradient Descent.\n",
    "print(\"\\nNormalizing features for Model V3 (Z-score normalization)...\")\n",
    "mu_v3     = np.mean(X_train_v3, axis=0)  # Calculate mean for each feature in the expanded X_train_v3\n",
    "sigma_v3  = np.std(X_train_v3, axis=0)   # Calculate standard deviation for each feature\n",
    "sigma_v3[sigma_v3 == 0] = 1e-10          # Handle potential division by zero\n",
    "\n",
    "X_norm_v3 = (X_train_v3 - mu_v3) / sigma_v3 # Apply Z-score normalization\n",
    "\n",
    "print(f\"X_norm_v3 shape after normalization: {X_norm_v3.shape}\")\n",
    "\n",
    "\n",
    "# 4. Run Gradient Descent with the new X_norm_v3\n",
    "# The Gradient Descent algorithm is executed again to train \"Model V3\" with the\n",
    "# now 7-dimensional feature space. It will find the optimal weights and bias\n",
    "# for this enhanced linear model.\n",
    "print(\"\\nRunning Gradient Descent for Model V3 (7 features)...\")\n",
    "n_features_v3 = X_norm_v3.shape[1] # Get the total number of features (now 7)\n",
    "w_in_v3 = np.zeros(n_features_v3)  # Initialize weights as zeros for all 7 features\n",
    "b_in_v3 = 0                        # Initialize bias as zero\n",
    "\n",
    "alpha_v3 = 1.0e-1 # Learning rate - might need fine-tuning for new feature sets\n",
    "num_iters_v3 = 1000 # Number of iterations\n",
    "\n",
    "# Call the gradient_descent function using the updated data and parameters.\n",
    "# (compute_cost_vectorized and compute_gradient_vectorized functions are defined earlier)\n",
    "w_final_v3, b_final_v3, hist_v3 = gradient_descent(X_norm_v3, y_train_v3, w_in_v3, b_in_v3,\n",
    "                                                compute_cost_vectorized,\n",
    "                                                compute_gradient_vectorized,\n",
    "                                                alpha_v3, num_iters_v3)\n",
    "\n",
    "print(f\"\\nFinal w for Model V3: {w_final_v3}\")\n",
    "print(f\"Final b for Model V3: {b_final_v3}\")\n",
    "\n",
    "\n",
    "# 5. Plot Cost History for Model V3\n",
    "# This plot visualizes the convergence of the cost function for Model V3.\n",
    "# A smooth, continuously decreasing curve indicates that Gradient Descent is\n",
    "# successfully minimizing the cost.\n",
    "print(\"\\nPlotting Cost vs. Iteration for Model V3...\")\n",
    "fig, (ax1_v3, ax2_v3) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1_v3.plot(hist_v3[:100],  color = 'mediumorchid') # Plot the initial iterations to show rapid decrease\n",
    "ax1_v3.set_title(\"Cost vs. Iteration (Model V3 - Start)\");\n",
    "ax1_v3.set_ylabel('Cost');\n",
    "ax1_v3.set_xlabel('Iteration Step');\n",
    "start_index_for_end_plot_v3 = 100\n",
    "ax2_v3.plot(start_index_for_end_plot_v3 + np.arange(len(hist_v3[start_index_for_end_plot_v3:])), hist_v3[start_index_for_end_plot_v3:], color = 'mediumorchid')\n",
    "ax2_v3.set_title(\"Cost vs. Iteration (Model V3 - End)\") # Plot later iterations to show fine-tuning/convergence\n",
    "ax2_v3.set_ylabel('Cost')\n",
    "ax2_v3.set_xlabel('Iteration Step')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 6. Predict and Plot with Final Model V3\n",
    "# This section generates predictions using the trained Model V3 and visualizes its\n",
    "# performance against the original features. This is a critical step to assess\n",
    "# the impact of the comprehensive feature engineering on the linear model's fit.\n",
    "\n",
    "#print(\"\\nGenerating predictions and plotting for Model V3...\")\n",
    "\n",
    "# Make predictions using the optimized Model V3 parameters (w_final_v3, b_final_v3)\n",
    "# and the normalized feature set (X_norm_v3).\n",
    "yp_final_v3 = X_norm_v3 @ w_final_v3 + b_final_v3 # Vectorized prediction for Model V3\n",
    "\n",
    "# Define custom colors for Model V3 plots. These are chosen for high contrast\n",
    "# and to differentiate this model's predictions from previous iterations.\n",
    "TARGET_COLOR_V3 = 'cadetblue' # Consistent target color (blue/gray)\n",
    "PREDICT_COLOR_V3 = 'firebrick' # Strong, contrasting red for Model V3 predictions\n",
    "\n",
    "# Plotting the initial 4 original features vs predictions from Model V3.\n",
    "# We use 'initial_feature_names' for the x-axis labels to maintain visual consistency\n",
    "# with previous plots, allowing for direct comparison of prediction patterns.\n",
    "fig,ax=plt.subplots(1,len(initial_feature_names),figsize=(16, 4),sharey=True)\n",
    "for i in range(len(initial_feature_names)): # Loop through the original 4 features for plotting\n",
    "    ax[i].scatter(X_train_v3[:,i],y_train_v3,\n",
    "                  label = 'Target', s=5, alpha=0.5, color=TARGET_COLOR_V3)\n",
    "    ax[i].scatter(X_train_v3[:,i],yp_final_v3,\n",
    "                  label = 'Predict (V3)', s=5, alpha=0.7, color=PREDICT_COLOR_V3)\n",
    "    ax[i].set_xlabel(initial_feature_names[i])\n",
    "    ax[i].set_title(initial_feature_names[i])\n",
    "\n",
    "ax[0].set_ylabel(\"Median House Value\");\n",
    "ax[0].legend();\n",
    "fig.suptitle(\"Target vs. Prediction: Model V3 (incl. Poly & Log Features)\", y=1.02)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# --- Optional: Explicitly plot MedInc (original) vs. predictions for Model V3 ---\n",
    "# This plot is crucial to visually confirm if the added polynomial and other terms\n",
    "# helped capture the non-linear relationship in 'MedInc' more effectively.\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_train_v3[:, medinc_original_column_index_v3], y_train_v3,\n",
    "            label='Target', s=5, alpha=0.5, color=TARGET_COLOR_V3)\n",
    "plt.scatter(X_train_v3[:, medinc_original_column_index_v3], yp_final_v3,\n",
    "            label='Predict (V3 with All Features)', s=5, alpha=0.7, color=PREDICT_COLOR_V3)\n",
    "plt.xlabel(\"MedInc (Original)\")\n",
    "plt.ylabel(\"Median House Value\")\n",
    "plt.title(f\"MedInc (Original) vs. Target/Prediction (Model V3)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# --- Additional Plot: Plot the new Log_Population feature if you want to see its effect ---\n",
    "# This plot visualizes the relationship between the transformed 'Log_Population' feature\n",
    "# and the target, showing how the model predicts on this new scale.\n",
    "log_pop_col_idx = X_features_v3.index('Log_Population') # Get the index of the Log_Population feature\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_train_v3[:, log_pop_col_idx], y_train_v3,\n",
    "            label='Target', s=5, alpha=0.5, color=TARGET_COLOR_V3)\n",
    "plt.scatter(X_train_v3[:, log_pop_col_idx], yp_final_v3,\n",
    "            label='Predict (V3 with All Features)', s=5, alpha=0.7, color=PREDICT_COLOR_V3)\n",
    "plt.xlabel(\"Log_Population\")\n",
    "plt.ylabel(\"Median House Value\")\n",
    "plt.title(f\"Log_Population vs. Target/Prediction (Model V3)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- End of Iteration 3 Analysis ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412e473-00f9-4f36-9c8c-c1506dbcd26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate RMSE: Measures the average magnitude of the errors. Lower is better.\n",
    "rmse_v1 = np.sqrt(mean_squared_error(y_train, yp))\n",
    "rmse_v2 = np.sqrt(mean_squared_error(y_train_v2, yp_final_v2))\n",
    "rmse_v3 = np.sqrt(mean_squared_error(y_train_v3, yp_final_v3))\n",
    "\n",
    "# Calculate R-squared: Represents the proportion of variance in the dependent variable\n",
    "# that is predictable from the independent variables. Higher (closer to 1) is better.\n",
    "r2_v1 = r2_score(y_train, yp)\n",
    "r2_v3 = r2_score(y_train_v3, yp_final_v3)\n",
    "r2_v2 = r2_score(y_train_v2, yp_final_v2)\n",
    "\n",
    "print(f\"\\n--- Performance of the Models (Linear Regression) ---\")\n",
    "print(f\" RMSE of Model 1: {rmse_v1:.4f}\")\n",
    "print(f\" R-squared of Model 1: {r2_v1:.4f}\")\n",
    "print(f\" RMSE of Model 2: {rmse_v2:.4f}\")\n",
    "print(f\" R-squared of Model 2: {r2_v2:.4f}\")\n",
    "print(f\" RMSE of Model 3: {rmse_v3:.4f}\")\n",
    "print(f\" R-squared of Model 3: {r2_v3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2395e4-1de7-45df-853c-011bc98834eb",
   "metadata": {},
   "source": [
    "## 6. Concluding Linear Regression Analysis\n",
    "Despite extensive feature engineering in Model V3, which included polynomial, logarithmic, and interaction terms, the Linear Regression model still exhibits inherent limitations. The visual analysis, especially for 'MedInc', indicates its struggle with highly non-linear relationships and varying data dispersion (heteroscedasticity). The linear model's fundamental assumption of linearity and homoscedasticity prevents it from fully capturing such complex patterns.\n",
    "\n",
    "While each iteration of feature engineering led to improvements in the overall fit (as seen by the decreasing cost and improved R-squared values within this notebook), the model's core constraints limit its ultimate predictive power for intricate data structures.\n",
    "\n",
    "Therefore, to potentially achieve a more accurate and robust model capable of handling these complex, non-linear relationships with ease, the **Random Forest Regressor** can be explored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
