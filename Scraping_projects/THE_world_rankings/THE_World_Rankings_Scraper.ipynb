{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63887da3-94a8-4212-8293-246eac1bfac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import selenium # Main Selenium library (though often 'from selenium import webdriver' is enough)\n",
    "from selenium import webdriver # The main Selenium WebDriver module\n",
    "from selenium.webdriver.common.by import By # To locate elements by various strategies (e.g., ID, CSS_SELECTOR, XPATH)\n",
    "from selenium.webdriver.support.ui import WebDriverWait # To wait for specific conditions on the webpage\n",
    "from selenium.webdriver.support import expected_conditions as EC # Predefined conditions for WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException # Specific exception for wait timeouts\n",
    "from bs4 import BeautifulSoup # Beautiful Soup for parsing HTML content\n",
    "import csv # For writing extracted data to CSV files\n",
    "import time # For time-related functions (e.g., delays)\n",
    "import traceback # For detailed error reporting\n",
    "import os # Import os for operating system-related functionalities (e.g., path manipulation, directory creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80987d1b-2854-443f-8239-ec6e6f5d4a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Base URL for the Times Higher Education (THE) World University Rankings.\n",
    "# It's templated with placeholders for year and subject.\n",
    "base_url = \"https://www.timeshighereducation.com/world-university-rankings/{}/subject-ranking/{}#!/length/-1/sort_by/rank/sort_order/asc/cols/scores\"\n",
    "\n",
    "# Define the range of years to scrape. (e.g., 2025 only for demonstration)\n",
    "years = range(2025, 2026)\n",
    "\n",
    "# Define the list of subjects to scrape.\n",
    "# Only arts-and-humanities is contained in the list for demonstration, \n",
    "#but other subjects can be uncommented and scraped simultaneously.\n",
    "subjects = [\"arts-and-humanities\"] # Example: \"engineering-and-it\", \"computer-science\", etc.\n",
    "\n",
    "# --- Selenium WebDriver Setup ---\n",
    "# Configure Chrome options for running the browser.\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run Chrome in headless mode (without a visible GUI).\n",
    "                                    # This is common for server-side scraping.\n",
    "# Note: Ensure you have the compatible chromedriver executable in your system's PATH\n",
    "# or in the same directory as this script.\n",
    "\n",
    "# Initialize the Chrome WebDriver.\n",
    "# The Service() class is used to manage the ChromeDriver executable.\n",
    "# If chromedriver is in PATH, you can just use `Service()`.\n",
    "# If it's in the current directory, `Service(executable_path='./chromedriver')` would work,\n",
    "# but if it's named 'chromedriver' and in PATH, this is fine.\n",
    "driver = webdriver.Chrome(options=options)\n",
    "print(\"Chrome WebDriver initialized.\")\n",
    "\n",
    "# --- Main Scraping Logic ---\n",
    "# Use a try-finally block to ensure the WebDriver is closed even if errors occur.\n",
    "try:\n",
    "    # Loop through each year and subject combination to construct URLs and scrape data.\n",
    "    for year in years:\n",
    "        for subject in subjects:\n",
    "            # Construct the full URL for the current year and subject.\n",
    "            url = base_url.format(year, subject)\n",
    "            print(f\"\\nScraping data for year: {year}, subject: {subject} from URL: {url}\")\n",
    "\n",
    "            # Instruct Selenium to load the URL in the browser. This executes JavaScript\n",
    "            # and renders the full page content.\n",
    "            driver.get(url)\n",
    "\n",
    "            # --- Initial Page Load Wait ---\n",
    "            # Wait for the main content area of the page to load, ensuring basic page structure is ready.\n",
    "            wait = WebDriverWait(driver, 15) # Initialize WebDriverWait with a timeout (15 seconds)\n",
    "            # wait = WebDriverWait(driver, 20) # Increase timeout if necessary for slow pages\n",
    "            wait.until(EC.presence_of_element_located((By.ID, \"main-content\")))\n",
    "            print(\"Main content loaded.\")\n",
    "\n",
    "            # --- Cookie Consent/Overlay Handling ---\n",
    "            # Websites often display cookie consent banners or other overlays that need to be dismissed\n",
    "            # to access the main content. This block attempts to find and remove such an overlay.\n",
    "            try:\n",
    "                # Wait for the cookie overlay to be present by its ID.\n",
    "                overlay = wait.until(EC.presence_of_element_located((By.ID, \"CybotCookiebotDialog\")))\n",
    "\n",
    "                # If the overlay is found, execute JavaScript to remove it from the DOM.\n",
    "                if overlay:\n",
    "                    driver.execute_script(\"document.getElementById('CybotCookiebotDialog').remove();\")\n",
    "                    print(\"Cookie overlay removed using JavaScript.\")\n",
    "                    # Add a small, fixed delay to allow the browser to fully process the JavaScript.\n",
    "                    time.sleep(2)\n",
    "                    # Check if the overlay is still present (in case of re-rendering or initial click failure)\n",
    "                    # This line (if overlay := ...) means it tries to find the element again after 2 seconds.\n",
    "                    # If it's still there, it tries to remove it again.\n",
    "                    # A more robust approach might be to click a \"reject\" or \"accept\" button if available.\n",
    "                    if overlay := wait.until(EC.presence_of_element_located((By.ID, \"CybotCookiebotDialog\"))):\n",
    "                        print(\"Overlay still present after first attempt. Trying removal again...\")\n",
    "                        driver.execute_script(\"document.getElementById('CybotCookiebotDialog').remove();\")\n",
    "\n",
    "            except TimeoutException:\n",
    "                # If the overlay is not found within the timeout, assume it's not present and proceed.\n",
    "                print(\"Cookie overlay not found. Continuing scraping.\")\n",
    "                pass # Proceed without handling the overlay\n",
    "            except Exception as e:\n",
    "                # Catch any other unexpected errors during overlay handling.\n",
    "                print(f\"Error handling cookie overlay: {e}\")\n",
    "\n",
    "            # --- Click on \"Scores\" Tab ---\n",
    "            # The website often has different views (e.g., 'Overview', 'Scores'). We click 'Scores'\n",
    "            # to ensure the ranking data is presented in a consistent format for extraction.\n",
    "            scores_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//label[@for='scores']\")))\n",
    "            scores_button.click()\n",
    "            print(\"Clicked 'Scores' tab.\")\n",
    "\n",
    "            # --- Data Reload Wait ---\n",
    "            # After clicking 'Scores', the table data reloads. A fixed delay is used here\n",
    "            # to give the page time to fully render the new data.\n",
    "            # (Note: Using explicit waits like EC.staleness_of(old_element) or EC.url_changes\n",
    "            # would generally be more robust than fixed time.sleep() calls.)\n",
    "            time.sleep(5)\n",
    "            # Wait for the main data table to be present and visible after the click.\n",
    "            table_element = wait.until(EC.presence_of_element_located((By.ID, 'datatable-1')))\n",
    "            print(\"Ranking table found.\")\n",
    "\n",
    "            # A second fixed delay before extracting content, ensuring all dynamic content is settled.\n",
    "            time.sleep(5)\n",
    "\n",
    "            # --- Extract HTML with BeautifulSoup ---\n",
    "            # Get the fully rendered HTML source code of the page from Selenium.\n",
    "            html = driver.page_source\n",
    "            # Create a BeautifulSoup object to parse the HTML.\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            # Find the main ranking table by its ID.\n",
    "            table = soup.find(\"table\", id =\"datatable-1\")\n",
    "\n",
    "            # --- Data Parsing and CSV Writing ---\n",
    "            # Check if the table and its body are found before proceeding.\n",
    "            if table:\n",
    "                table_body = table.find(\"tbody\")\n",
    "                if table_body:\n",
    "                    # The header row is assumed to be part of the tbody for parsing logic.\n",
    "                    # This scraper handles variations in row structure ('institution-disabled' vs. regular).\n",
    "                    header_row = table_body.find(\"tr\") # This is the first row in tbody, assumed to be first university data\n",
    "\n",
    "                    if header_row: # Proceed if at least one data row (or header-like row) is found\n",
    "                        # Construct the output CSV filename based on subject and year.\n",
    "                        # Files will be saved in the same directory as the script by default.\n",
    "                        filename = f'{subject}_rankings_{year}.csv'\n",
    "                        # Create a directory named 'THE_rankings_raw_data' to store the CSVs if it doesn't exist\n",
    "                        output_folder = \"THE_rankings_raw_data\"\n",
    "                        if not os.path.exists(output_folder):\n",
    "                            os.makedirs(output_folder)\n",
    "                            print(f\"Created output directory: {output_folder}\")\n",
    "                        output_filepath = os.path.join(output_folder, filename)\n",
    "\n",
    "\n",
    "                        with open(output_filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                            writer = csv.writer(csvfile) # Create a CSV writer object\n",
    "                            # Define the CSV header based on the columns expected from the website.\n",
    "                            writer.writerow(['Rank', 'Name', 'Country/Region', 'Overall', 'Research Quality', 'Industry Income', 'International Outlook', 'Research Environment', 'Teaching'])\n",
    "\n",
    "                            # Iterate through each table row (<tr>) in the table body.\n",
    "                            for row in table_body.find_all(\"tr\"):\n",
    "                                data = [] # List to hold extracted data for the current row\n",
    "                                cells = row.find_all(\"td\") # Find all table data cells (<td>) in the row\n",
    "\n",
    "                                # --- Handle \"institution-disabled\" rows ---\n",
    "                                # Some rows might have a specific class indicating they are \"disabled\"\n",
    "                                # (e.g., hidden from ranking or subscription-only). They have a different HTML structure.\n",
    "                                if \"institution-disabled\" in row.get(\"class\", []):\n",
    "                                    # Extract data using specific class names within <td> for disabled rows.\n",
    "                                    data.append(cells[0].text.strip()) # Rank (from first td)\n",
    "                                    name_div = row.find(\"div\", class_=\"ranking-institution-title\")\n",
    "                                    data.append(name_div.text.strip() if name_div else \"\") # Name\n",
    "                                    location_div = row.find(\"div\", class_=\"ranking-institution__disabled-location\")\n",
    "                                    data.append(location_div.text.strip() if location_div else \"\") # Country/Region\n",
    "                                    # Extract specific scores by their class names\n",
    "                                    overall_score_div = row.find(\"td\", class_=\"scores overall-score\")\n",
    "                                    data.append(overall_score_div.text.strip() if overall_score_div else \"\")\n",
    "                                    citations_div = row.find(\"td\", class_=\"scores citations-score\")\n",
    "                                    data.append(citations_div.text.strip() if citations_div else \"\")\n",
    "                                    industry_div = row.find(\"td\", class_=\"scores industry_income-score\")\n",
    "                                    data.append(industry_div.text.strip() if industry_div else \"\")\n",
    "                                    int_outlook_div = row.find(\"td\", class_=\"scores international_outlook-score\")\n",
    "                                    data.append(int_outlook_div.text.strip() if int_outlook_div else \"\")\n",
    "                                    research_score_div = row.find(\"td\", class_=\"scores research-score\")\n",
    "                                    data.append(research_score_div.text.strip() if research_score_div else \"\")\n",
    "                                    teaching_score_div = row.find(\"td\", class_=\"scores teaching-score\")\n",
    "                                    data.append(teaching_score_div.text.strip() if teaching_score_div else \"\")\n",
    "                                else:\n",
    "                                    # --- Handle Regular Data Rows ---\n",
    "                                    # For regular rows (not disabled), extract data directly from <td> elements.\n",
    "                                    # This assumes a consistent order and presence of 8 cells.\n",
    "                                    if len(cells) >= 8: # Ensure enough cells are present\n",
    "                                        data.append(cells[0].text.strip()) # Rank\n",
    "                                        data.append(cells[1].find(\"a\").text.strip()) # Name (from <a> tag within 2nd td)\n",
    "                                        location_div = cells[1].find(\"div\", class_=\"location\") # Location (from <div> within 2nd td)\n",
    "                                        country_region = location_div.text.strip() if location_div else \"\"\n",
    "                                        data.append(country_region)\n",
    "                                        data.append(cells[2].text.strip()) # Overall score\n",
    "                                        data.append(cells[3].text.strip()) # Research Quality (formerly Citations)\n",
    "                                        data.append(cells[4].text.strip()) # Industry Income\n",
    "                                        data.append(cells[5].text.strip()) # International Outlook\n",
    "                                        data.append(cells[6].text.strip()) # Research Environment\n",
    "                                        data.append(cells[7].text.strip()) # Teaching\n",
    "                                    else:\n",
    "                                        print(f\"Skipping row with insufficient data cells: {row.text.strip()}\")\n",
    "                                        continue # Skip this row and move to the next\n",
    "\n",
    "                                writer.writerow(data) # Write the extracted data row to the CSV file\n",
    "\n",
    "                    else:\n",
    "                        print(\"Error: Could not find any data rows/header in the table body.\")\n",
    "                else:\n",
    "                    print(\"Error: Could not find the table body (tbody).\")\n",
    "            else:\n",
    "                print(\"Error: Could not find the main ranking table (id='datatable-1').\")\n",
    "\n",
    "            # Add a small delay between scraping different years/subjects to be polite to the website.\n",
    "            time.sleep(2)\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch any unexpected errors that occur during the scraping process.\n",
    "    print(f\"An unhandled error occurred during scraping: {e}\")\n",
    "    print(\"Traceback details:\")\n",
    "    print(traceback.format_exc()) # Print full traceback for debugging\n",
    "finally:\n",
    "    # Ensure the Selenium WebDriver is closed cleanly, regardless of whether errors occurred.\n",
    "    if 'driver' in locals() and driver: # Check if driver was successfully initialized\n",
    "        driver.quit() # Close the browser and terminate the WebDriver session\n",
    "        print(\"Selenium WebDriver closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97f15c-a90c-435b-a6c4-8a05643bce6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
