{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d62c3c-4393-46ac-992b-1e1b938d4a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scrapy Scraper to extract QS World University Rankings data.\n",
    "\n",
    "This spider integrates Selenium WebDriver to handle JavaScript rendering and dynamic content\n",
    "loading on the QS World University Rankings website. It navigates through different\n",
    "subjects and years, extracts university ranking data, and saves it to CSV files.\n",
    "\n",
    "Note on Asynchronous Execution & Twisted Compatibility:\n",
    "Running Scrapy (which utilizes Twisted for asynchronous operations) within interactive\n",
    "environments like Jupyter Notebooks/Lab can lead to a \"RuntimeError: Event loop is already running\".\n",
    "This is because Jupyter already has an active asyncio event loop. The `nest_asyncio.apply()`\n",
    "patch is used at the start of this script to allow for nested event loops, resolving this specific issue.\n",
    "\n",
    "This spider has been successfully tested in environments with Twisted version 23.x.x.\n",
    "However, if you encounter other unexpected runtime or compatibility errors while running this spider,\n",
    "**particularly on macOS or Linux environments**, it may be related to specific Twisted versions.\n",
    "As a troubleshooting step, you might consider trying Twisted==22.10.0, as this version has historically\n",
    "shown strong stability with Scrapy on these operating systems (Windows environments typically experience\n",
    "fewer such issues).\n",
    "\"\"\"\n",
    "import scrapy\n",
    "from scrapy.selector import Selector        # For parsing HTML content using CSS/XPath selectors\n",
    "from scrapy.crawler import CrawlerProcess   # To run Scrapy spiders programmatically\n",
    "from selenium import webdriver              # The main Selenium WebDriver module\n",
    "from selenium.webdriver.chrome.options import Options   # To configure Chrome browser options\n",
    "from selenium.webdriver.common.by import By # To locate elements by various strategies (e.g., ID, CSS_SELECTOR, XPATH)\n",
    "from selenium.webdriver.support.ui import WebDriverWait # To wait for specific conditions on the webpage\n",
    "from selenium.webdriver.common.action_chains import ActionChains # For performing complex user interactions (e.g., mouse hovers, clicks)\n",
    "from selenium.webdriver.support import expected_conditions as EC # Predefined conditions for WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service   # To manage the ChromeDriver executable\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, StaleElementReferenceException, ElementClickInterceptedException # Common Selenium exceptions for robust error handling\n",
    "import csv   # For writing extracted data to CSV files\n",
    "import time  # For time-related functions (e.g., sleeps, though generally avoided in Scrapy)\n",
    "import os    # For operating system-related functions (e.g., path manipulation, checking OS type)\n",
    "import random        # For generating random numbers (e.g., for delays, user agents) - not explicitly used here but good for anti-bot\n",
    "import nest_asyncio  # To allow nested asyncio event loops, crucial for running Scrapy/Twisted in Jupyter/IPython\n",
    "nest_asyncio.apply() # Apply the patch to allow nested event loops. This resolves RuntimeErrors in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a1b75-491e-4da3-a861-cc40255aa244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UniversityRankingSpider(scrapy.Spider):\n",
    "    \"\"\"\n",
    "    A Scrapy spider to scrape university rankings data from the QS World University Rankings website.\n",
    "    This spider integrates Selenium to handle JavaScript rendering and dynamic content loading.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"university_ranking\" # Unique name for the spider, used to run it from the command line\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the spider and sets up the Selenium webdriver.\n",
    "        Configures Chrome options for headless mode, user agent, and logging.\n",
    "        Handles platform-specific ChromeDriver executable paths.\n",
    "        \"\"\"\n",
    "        super().__init__() # Call the parent class (scrapy.Spider) __init__ method\n",
    "\n",
    "        # --- Selenium WebDriver Configuration ---\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")    # Run Chrome in headless mode (without a visible GUI)\n",
    "        chrome_options.add_argument(\"--disable-gpu\") # Recommended for headless mode, especially on Windows\n",
    "        chrome_options.add_argument(\"--no-sandbox\")  # Recommended for headless mode in some environments (e.g., Docker)\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\") # Recommended to prevent issues in constrained environments\n",
    "\n",
    "        # User-Agent: Mimic a real browser to avoid detection.\n",
    "        # It's good practice to rotate these or use a realistic one.\n",
    "        chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\")\n",
    "\n",
    "        # Suppress logging to avoid excessive console output from ChromeDriver\n",
    "        chrome_options.add_argument(\"--log-level=3\") # INFO, WARNING, ERROR, FATAL (3 is FATAL, 0 is ALL)\n",
    "\n",
    "        # Exclude automation detection flags (experimental options)\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "        # DesiredCapabilities are largely deprecated with newer Selenium versions and ChromeOptions\n",
    "        # caps = DesiredCapabilities.CHROME # Deprecated\n",
    "        # caps['goog:loggingPrefs'] = {'performance': 'ALL'} # Deprecated - use chrome_options\n",
    "        # caps = {} # You can define capabilities as an empty dictionary if needed for other settings\n",
    "\n",
    "        # --- ChromeDriver Path Configuration (Cross-Platform Robustness) ---\n",
    "        # Dynamically determine the correct ChromeDriver executable name based on the operating system.\n",
    "        # This makes the script more portable across different OS.\n",
    "        if os.name == 'nt':  # Windows\n",
    "            chrome_driver_executable = 'chromedriver.exe'\n",
    "        elif os.name == 'posix':  # macOS or Linux\n",
    "            chrome_driver_executable = 'chromedriver'  # No extension on macOS/Linux\n",
    "        else:\n",
    "            raise OSError(f\"Unsupported operating system: {os.name}\")\n",
    "\n",
    "        # Construct the full path to ChromeDriver.\n",
    "        # It's assumed that 'chromedriver' is placed in a directory that is part of the system's PATH\n",
    "        # (e.g., /usr/local/bin/ on macOS/Linux, or in the script's directory for Windows).\n",
    "        # For robustness, it's often better to specify the full path if not in PATH, or rely on PATH.\n",
    "        # If ChromeDriver is in PATH, you can often just do `Service()` without `executable_path`.\n",
    "        # However, for explicit control, using `executable_path` is good.\n",
    "        # For this setup, we assume it's in the current working directory for simplicity of path construction.\n",
    "        # If it's in /usr/local/bin, you might just use: service = Service()\n",
    "        # Or explicitly: service = Service(executable_path=\"/usr/local/bin/chromedriver\")\n",
    "        chrome_driver_path = os.path.join(os.getcwd(), chrome_driver_executable) # Assumes chromedriver is in current working dir\n",
    "\n",
    "        # Use the Service class to manage ChromeDriver.\n",
    "        # This is the modern way to specify the executable path for the driver.\n",
    "        service = Service(executable_path=chrome_driver_path)\n",
    "        self.driver = webdriver.Chrome(service=service, options=chrome_options)  # Initialize Chrome WebDriver instance\n",
    "        self.driver.maximize_window() # Maximize the browser window (even in headless mode, can affect rendering)\n",
    "        self.base_url = \"https://www.topuniversities.com/university-subject-rankings/{subject}/{year}?items_per_page=150&tab=indicators&sort_by=rank&order_by=asc\"  # Define the base URL template\n",
    "\n",
    "    def start_requests(self):\n",
    "        \"\"\"\n",
    "        Generates the initial requests to start scraping.\n",
    "        This method is called by Scrapy when the spider starts.\n",
    "        It iterates through predefined subjects and years to construct the URLs for ranking pages.\n",
    "        \"\"\"\n",
    "        # Define the subjects to scrape. Commented out subjects can be uncommented to expand scraping.\n",
    "        # Limiting subjects/years is useful during development to avoid hitting rate limits.\n",
    "        subjects = [ # works best for two subjects at a time\n",
    "            # \"arts-and-humanities\",\n",
    "            # \"engineering-technology\",\n",
    "            # \"life-sciences-medicine\",\n",
    "            # \"natural-sciences\",\n",
    "            \"theology-divinity-religious-studies\"\n",
    "            # \"veterinary-science\",\n",
    "        ]\n",
    "        years = range(2024, 2025)  # Define the range of years to scrape (e.g., 2024 only)\n",
    "\n",
    "        for subject in subjects:\n",
    "            for year in years:\n",
    "                url = self.base_url.format(subject=subject, year=year) # Construct the full URL using subject and year\n",
    "                # Yield a Scrapy Request. The 'parse' method will be called to handle the response.\n",
    "                # 'meta' is used to pass additional data (subject and year) to the callback method.\n",
    "                yield scrapy.Request(\n",
    "                    url=url,\n",
    "                    callback=self.parse,\n",
    "                    meta={'subject': subject, 'year': year}\n",
    "                )\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        Parses the HTML response for each page using Selenium for rendering,\n",
    "        extracts university ranking data, and handles pagination to scrape multiple pages.\n",
    "\n",
    "        Args:\n",
    "            response (scrapy.http.Response): The Scrapy Response object for the current URL.\n",
    "        \"\"\"\n",
    "        self.driver.get(response.url)  # Instruct Selenium to load the URL. This executes JavaScript.\n",
    "\n",
    "        # --- Pop-up Handling (Cookies and Survey) ---\n",
    "        # Websites often have pop-ups that block content. These blocks attempt to remove them\n",
    "        # using JavaScript execution via Selenium. WebDriverWait ensures the pop-up is present\n",
    "        # before attempting to remove it, making the script more robust.\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until( # Wait up to 10 seconds\n",
    "                EC.presence_of_element_located((By.ID, 'sliding-popup')) # Wait for the cookie consent pop-up by its ID\n",
    "            )\n",
    "            self.driver.execute_script(\"document.getElementById('sliding-popup').remove();\") # Execute JavaScript to remove the element\n",
    "            print(\"Cookie block removed using JavaScript.\")\n",
    "        except TimeoutException: # Catch TimeoutException if the element doesn't appear within 10s\n",
    "            print(\"Cookie block not found or timed out.\")\n",
    "        except Exception as e: # Catch any other general exceptions\n",
    "            print(f\"Error closing cookie block: {e}\")\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until( # Wait up to 10 seconds\n",
    "                EC.presence_of_element_located((By.ID, 'surveyModal')) # Wait for the survey pop-up by its ID\n",
    "            )\n",
    "            self.driver.execute_script(\"document.getElementById('surveyModal').remove();\") # Execute JavaScript to remove the element\n",
    "            print(\"Survey popup removed using JavaScript.\")\n",
    "        except TimeoutException: # Catch TimeoutException if the element doesn't appear within 10s\n",
    "            print(\"Survey popup not found or timed out.\")\n",
    "        except Exception as e: # Catch any other general exceptions\n",
    "            print(f\"Error closing survey popup: {e}\")\n",
    "\n",
    "        # --- File Path Definition ---\n",
    "        # Construct the output file path for the current subject and year.\n",
    "        subject = response.meta['subject']\n",
    "        year = response.meta['year']\n",
    "        folder_name = 'rankings_csv' # Name of the folder to store CSVs\n",
    "        current_directory = os.getcwd() # Get the current working directory\n",
    "        folder_path = os.path.join(current_directory, folder_name) # Full path to the output folder\n",
    "\n",
    "        # Create the output folder if it doesn't exist\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"Created output directory: {folder_path}\")\n",
    "\n",
    "        filename = f'QS_Rankings_{subject}-{year}.csv' # Construct the filename (e.g., 'QS_Rankings_theology-divinity-religious-studies-2024.csv')\n",
    "        file_path = os.path.join(folder_path, filename) # Full path to the CSV file\n",
    "\n",
    "        # Open the CSV file in write mode. 'w' creates/overwrites, 'encoding' for proper characters, 'newline' to prevent blank rows.\n",
    "        with open(file_path, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.writer(f) # Create a CSV writer object\n",
    "            header = [ # Define the CSV header row\n",
    "                'Rank', 'Name', 'Location', 'Employer Reputation',\n",
    "                'H-index Citations', 'Citations per Paper',\n",
    "                'Academic Reputation', 'Global Engagement'\n",
    "            ]\n",
    "            writer.writerow(header) # Write the header row to the CSV\n",
    "\n",
    "            page_number = 0 # Initialize page number for tracking pagination\n",
    "\n",
    "            # --- Pagination Loop ---\n",
    "            # Loop to navigate through multiple pages of rankings until no more \"Next\" button is found.\n",
    "            while True:\n",
    "                # Wait for the main ranking data rows to be present on the page.\n",
    "                # This ensures the JavaScript content has loaded before attempting to parse.\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 15).until(\n",
    "                        EC.presence_of_element_located(\n",
    "                            (By.CSS_SELECTOR, \"div._qs-ranking-data-row\"))) # Wait for a specific div element to be 'viewable'\n",
    "                except TimeoutException:\n",
    "                    print(f\"Timeout waiting for ranking data on page {page_number}. Exiting pagination.\")\n",
    "                    break # Exit loop if data rows don't load\n",
    "\n",
    "                # Get the fully rendered HTML content from Selenium.\n",
    "                rendered_html = self.driver.page_source\n",
    "\n",
    "                # Create a Scrapy Selector from the rendered HTML.\n",
    "                # This allows using Scrapy's powerful CSS/XPath selectors on the Selenium-rendered page.\n",
    "                scrapy_response = Selector(text=rendered_html)\n",
    "\n",
    "                # --- Data Extraction ---\n",
    "                # Select university rows and indicator rows.\n",
    "                # 'div.hide-this-in-mobile-indi' targets the desktop view.\n",
    "                university_rows = scrapy_response.css(\n",
    "                    'div.hide-this-in-mobile-indi div._qs-ranking-data-row') # Selects the div containing university ranking data\n",
    "                indicator_rows = scrapy_response.css(\n",
    "                    'div.hide-this-in-mobile-indi div.col-lg-6:not(._right_background)') # Selects the div containing indicator data\n",
    "\n",
    "                if not university_rows: # Check if no university rows were found on the current page\n",
    "                    print(f\"No university data found on page {page_number}. Assuming end of results.\")\n",
    "                    break # Exit loop if no data is found\n",
    "\n",
    "                for i, row in enumerate(university_rows): # Iterate through each university row\n",
    "                    data = {} # Dictionary to store extracted data for the current university\n",
    "                    data['Rank'] = row.css('div._univ-rank::text').get() # Extracts the university rank\n",
    "                    info_div = row.css('div.col-lg-8') # Select the info div for name and location\n",
    "                    data['Name'] = info_div.css('a.uni-link::text').get() # Extracts the university name\n",
    "                    data['Location'] = info_div.css('div.location::text').get() # Extracts the university location\n",
    "\n",
    "                    # --- Access the corresponding indicator row using the index ---\n",
    "                    # The indicator data (scores) is in a separate div, but corresponds to the university row by index.\n",
    "                    indicator_row = indicator_rows[i] # Access the indicator row using the same index\n",
    "\n",
    "                    # Wait for the indicator score blocks to be present.\n",
    "                    # This is crucial as scores might load dynamically after the main university list.\n",
    "                    WebDriverWait(self.driver, 20).until(\n",
    "                        EC.presence_of_all_elements_located(\n",
    "                            (By.CSS_SELECTOR, \"div._smallblocksfix-width\")))\n",
    "\n",
    "                    # --- Extract ranking scores with specific XPATHs ---\n",
    "                    # XPATHs are used here to precisely target the score elements.\n",
    "                    # NB: The XPATHs use 'trade_ranking_col_no_X' and then index [1], [2], [3].\n",
    "                    # This often means the order of scores can vary by subject/year.\n",
    "                    data['Employer Reputation'] = indicator_row.xpath(\n",
    "                        \".//div[contains(@class, 'trade_ranking_col_no_0')]/div/div/span/div/div/text()\"\n",
    "                    ).get()\n",
    "                    data['H-index Citations'] = indicator_row.xpath(\n",
    "                        \".//div[contains(@class, 'trade_ranking_col_no_1')][3]/div/div/span/div/div/text()\"\n",
    "                    ).get()\n",
    "                    data['Citations per Paper'] = indicator_row.xpath(\n",
    "                        \".//div[contains(@class, 'trade_ranking_col_no_1')][2]/div/div/span/div/div/text()\"\n",
    "                    ).get()\n",
    "                    data['Academic Reputation'] = indicator_row.xpath(\n",
    "                        \".//div[contains(@class, 'trade_ranking_col_no_1')][1]/div/div/span/div/div/text()\"\n",
    "                    ).get()\n",
    "                    data['Global Engagement'] = indicator_row.xpath(\n",
    "                        \".//div[contains(@class, 'trade_ranking_col_no_2')]/div/div/span/div/div/text()\"\n",
    "                    ).get()\n",
    "\n",
    "                    # Write extracted data to CSV in the predefined header order.\n",
    "                    writer.writerow([data.get(key, '') for key in header]) # Use .get(key, '') to handle missing keys gracefully\n",
    "                    print(\"Extracted:\", data.get('Name', 'N/A'), \"Rank:\", data.get('Rank', 'N/A')) # Print for verification\n",
    "\n",
    "                # --- Pagination Logic ---\n",
    "                # Attempt to click the \"Next\" button to navigate to the next page of rankings.\n",
    "                page_number += 1\n",
    "                try:\n",
    "                    next_button = WebDriverWait(self.driver, 10).until(\n",
    "                        EC.element_to_be_clickable(\n",
    "                            (By.XPATH, \"//a[@class='page-link next']//i[@class='fal fa-chevron-right']\") # XPATH for the next button\n",
    "                        )\n",
    "                    )\n",
    "                    current_url = self.driver.current_url # Store current URL to wait for it to change\n",
    "                    next_button.click() # Click the next button\n",
    "                    # Wait for the URL to change, indicating a successful page navigation.\n",
    "                    WebDriverWait(self.driver, 15).until(EC.url_changes(current_url))\n",
    "                    print(f\"Navigated to page {page_number}.\")\n",
    "\n",
    "                except TimeoutException:\n",
    "                    print(f\"No 'Next' button found on page {page_number}. Reached the end of pagination.\")\n",
    "                    break # Exit the loop if the \"Next\" button is not found or clickable within the timeout\n",
    "                except StaleElementReferenceException:\n",
    "                    print(f\"Stale element on page {page_number}. Retrying or breaking.\")\n",
    "                    break # Break if element becomes stale (page reloaded unexpectedly)\n",
    "                except ElementClickInterceptedException:\n",
    "                    print(f\"Click intercepted on page {page_number}. Another element is covering the button.\")\n",
    "                    break # Break if something is blocking the click\n",
    "\n",
    "    def closed(self, reason):\n",
    "        \"\"\"\n",
    "        Closes the Selenium webdriver when the spider finishes running or is closed for any reason.\n",
    "        This is crucial to ensure that the browser process is terminated and resources are released.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'driver') and self.driver: # Check if driver was initialized\n",
    "            self.driver.quit()  # Close the Selenium driver when the spider closes\n",
    "            print(\"Selenium WebDriver closed.\")\n",
    "\n",
    "# --- Scrapy Crawler Process Execution ---\n",
    "# This block allows the spider to be run directly from a Python script or Jupyter Notebook.\n",
    "process = CrawlerProcess()          # Initializes the Scrapy CrawlerProcess\n",
    "process.crawl(UniversityRankingSpider) # Tells the crawler to run the defined spider\n",
    "process.start()                     # Starts the crawling process. This will block until the crawling is finished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
